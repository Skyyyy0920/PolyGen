#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªé€‚åº”èåˆæ¨¡å—

åŠ¨æ€è°ƒæ•´Mayo-Lewisç†è®ºåˆ†å¸ƒä¸æ®‹å·®ä¿®æ­£åˆ†å¸ƒçš„æƒé‡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Tuple


class AdaptiveFusionModule(nn.Module):
    """
    è‡ªé€‚åº”èåˆæ¨¡å—
    
    æ ¹æ®æ¡ä»¶ç‰¹å¾å’Œç†è®ºç½®ä¿¡åº¦ï¼ŒåŠ¨æ€è°ƒæ•´ç†è®ºåˆ†å¸ƒä¸æ®‹å·®ä¿®æ­£åˆ†å¸ƒçš„æƒé‡
    """
    
    def __init__(self,
                 cond_dim: int,
                 mayo_param_dim: int = 6,  # f_A, f_B, p_AA, p_BB, p_AB, p_BA
                 hidden_dim: int = 128,
                 num_layers: int = 3,
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”èåˆæ¨¡å—
        
        Args:
            cond_dim: æ¡ä»¶ç‰¹å¾ç»´åº¦
            mayo_param_dim: Mayo-Lewiså‚æ•°ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            num_layers: MLPå±‚æ•°
            dropout: Dropoutç‡
        """
        super().__init__()
        
        self.cond_dim = cond_dim
        self.mayo_param_dim = mayo_param_dim
        self.hidden_dim = hidden_dim
        
        # ç½®ä¿¡åº¦é¢„æµ‹ç½‘ç»œ
        confidence_layers = []
        input_dim = cond_dim + mayo_param_dim
        
        for i in range(num_layers):
            if i == 0:
                confidence_layers.extend([
                    nn.Linear(input_dim, hidden_dim),
                    nn.LayerNorm(hidden_dim),
                    nn.GELU(),
                    nn.Dropout(dropout)
                ])
            elif i == num_layers - 1:
                confidence_layers.append(nn.Linear(hidden_dim, 1))
            else:
                confidence_layers.extend([
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.LayerNorm(hidden_dim),
                    nn.GELU(),
                    nn.Dropout(dropout)
                ])
        
        self.confidence_predictor = nn.Sequential(*confidence_layers)
        
        # åˆ†å¸ƒè´¨é‡è¯„ä¼°ç½‘ç»œ
        self.quality_assessor = nn.Sequential(
            nn.Linear(cond_dim + mayo_param_dim + 2,  # +2 for distribution statistics
            hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # æƒé‡è°ƒåˆ¶ç½‘ç»œ
        self.weight_modulator = nn.Sequential(
            nn.Linear(cond_dim + mayo_param_dim + 1,  # +1 for confidence
            hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 3),  # è¾“å‡º3ä¸ªæƒé‡ï¼štheoretical, residual, combined
            nn.Softmax(dim=-1)
        )
    
    def extract_mayo_lewis_features(self, mayo_params_batch: list) -> torch.Tensor:
        """
        ä»Mayo-Lewiså‚æ•°å­—å…¸ä¸­æå–ç‰¹å¾å¼ é‡
        
        Args:
            mayo_params_batch: Mayo-Lewiså‚æ•°å­—å…¸åˆ—è¡¨
            
        Returns:
            Mayo-Lewisç‰¹å¾å¼ é‡ [batch_size, mayo_param_dim]
        """
        features = []
        
        for params in mayo_params_batch:
            param_vector = [
                params.get('f_A', 0.5),
                params.get('f_B', 0.5),
                params.get('p_AA', 0.3),
                params.get('p_BB', 0.3),
                params.get('p_AB', 0.2),
                params.get('p_BA', 0.2)
            ]
            features.append(param_vector)
        
        return torch.tensor(features, dtype=torch.float32)
    
    def compute_distribution_statistics(self, 
                                      theoretical_dist: torch.Tensor,
                                      residual_dist: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—åˆ†å¸ƒç»Ÿè®¡é‡
        
        Args:
            theoretical_dist: ç†è®ºåˆ†å¸ƒ [batch_size, bins]
            residual_dist: æ®‹å·®ä¿®æ­£åˆ†å¸ƒ [batch_size, bins]
            
        Returns:
            åˆ†å¸ƒç»Ÿè®¡é‡ [batch_size, 2]
        """
        # è®¡ç®—åˆ†å¸ƒçš„ç†µ
        theoretical_entropy = -torch.sum(
            theoretical_dist * torch.log(theoretical_dist + 1e-8), dim=1
        )
        residual_entropy = -torch.sum(
            residual_dist * torch.log(residual_dist + 1e-8), dim=1
        )
        
        return torch.stack([theoretical_entropy, residual_entropy], dim=1)
    
    def predict_confidence(self, 
                          cond: torch.Tensor,
                          mayo_params_batch: list) -> torch.Tensor:
        """
        é¢„æµ‹Mayo-Lewisç†è®ºçš„ç½®ä¿¡åº¦
        
        Args:
            cond: æ¡ä»¶ç‰¹å¾ [batch_size, cond_dim]
            mayo_params_batch: Mayo-Lewiså‚æ•°å­—å…¸åˆ—è¡¨
            
        Returns:
            ç½®ä¿¡åº¦ [batch_size]
        """
        device = cond.device
        
        # æå–Mayo-Lewisç‰¹å¾
        mayo_features = self.extract_mayo_lewis_features(mayo_params_batch).to(device)
        
        # ç»„åˆç‰¹å¾
        combined_features = torch.cat([cond, mayo_features], dim=1)
        
        # é¢„æµ‹ç½®ä¿¡åº¦
        confidence_logits = self.confidence_predictor(combined_features)
        confidence = torch.sigmoid(confidence_logits.squeeze(-1))
        
        return confidence
    
    def assess_distribution_quality(self,
                                  cond: torch.Tensor,
                                  mayo_params_batch: list,
                                  theoretical_dist: torch.Tensor,
                                  residual_dist: torch.Tensor) -> torch.Tensor:
        """
        è¯„ä¼°åˆ†å¸ƒè´¨é‡
        
        Args:
            cond: æ¡ä»¶ç‰¹å¾
            mayo_params_batch: Mayo-Lewiså‚æ•°
            theoretical_dist: ç†è®ºåˆ†å¸ƒ
            residual_dist: æ®‹å·®ä¿®æ­£åˆ†å¸ƒ
            
        Returns:
            è´¨é‡è¯„åˆ† [batch_size]
        """
        device = cond.device
        
        # æå–ç‰¹å¾
        mayo_features = self.extract_mayo_lewis_features(mayo_params_batch).to(device)
        dist_stats = self.compute_distribution_statistics(theoretical_dist, residual_dist)
        
        # ç»„åˆç‰¹å¾
        combined_features = torch.cat([cond, mayo_features, dist_stats], dim=1)
        
        # è¯„ä¼°è´¨é‡
        quality_score = self.quality_assessor(combined_features).squeeze(-1)
        
        return quality_score
    
    def compute_adaptive_weights(self,
                               cond: torch.Tensor,
                               mayo_params_batch: list,
                               confidence: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—è‡ªé€‚åº”æƒé‡
        
        Args:
            cond: æ¡ä»¶ç‰¹å¾
            mayo_params_batch: Mayo-Lewiså‚æ•°
            confidence: ç½®ä¿¡åº¦
            
        Returns:
            æƒé‡ [batch_size, 3] - [theoretical_weight, residual_weight, combined_weight]
        """
        device = cond.device
        
        # æå–Mayo-Lewisç‰¹å¾
        mayo_features = self.extract_mayo_lewis_features(mayo_params_batch).to(device)
        
        # ç»„åˆç‰¹å¾
        combined_features = torch.cat([cond, mayo_features, confidence.unsqueeze(-1)], dim=1)
        
        # è®¡ç®—æƒé‡
        weights = self.weight_modulator(combined_features)
        
        return weights
    
    def forward(self,
                theoretical_dist: torch.Tensor,
                residual_corrected_dist: torch.Tensor,
                cond: torch.Tensor,
                mayo_params_batch: list,
                fusion_strategy: str = 'adaptive') -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ - è‡ªé€‚åº”èåˆåˆ†å¸ƒ
        
        Args:
            theoretical_dist: Mayo-Lewisç†è®ºåˆ†å¸ƒ [batch_size, bins]
            residual_corrected_dist: æ®‹å·®ä¿®æ­£åˆ†å¸ƒ [batch_size, bins]
            cond: æ¡ä»¶ç‰¹å¾ [batch_size, cond_dim]
            mayo_params_batch: Mayo-Lewiså‚æ•°å­—å…¸åˆ—è¡¨
            fusion_strategy: èåˆç­–ç•¥ ('adaptive', 'confidence', 'weighted')
            
        Returns:
            èåˆç»“æœå­—å…¸
        """
        # 1. é¢„æµ‹ç½®ä¿¡åº¦
        confidence = self.predict_confidence(cond, mayo_params_batch)
        
        # 2. è¯„ä¼°åˆ†å¸ƒè´¨é‡
        quality_score = self.assess_distribution_quality(
            cond, mayo_params_batch, theoretical_dist, residual_corrected_dist
        )
        
        # 3. æ ¹æ®ç­–ç•¥è¿›è¡Œèåˆ
        if fusion_strategy == 'confidence':
            # ç®€å•çš„ç½®ä¿¡åº¦åŠ æƒ
            confidence_weight = confidence.unsqueeze(-1)  # [B, 1]
            fused_dist = (confidence_weight * theoretical_dist + 
                         (1 - confidence_weight) * residual_corrected_dist)
            
        elif fusion_strategy == 'weighted':
            # åŸºäºè´¨é‡çš„åŠ æƒ
            quality_weight = quality_score.unsqueeze(-1)  # [B, 1]
            fused_dist = (quality_weight * theoretical_dist + 
                         (1 - quality_weight) * residual_corrected_dist)
            
        elif fusion_strategy == 'adaptive':
            # è‡ªé€‚åº”æƒé‡èåˆ
            weights = self.compute_adaptive_weights(cond, mayo_params_batch, confidence)
            
            # ä¸‰ç§åˆ†å¸ƒçš„åŠ æƒç»„åˆ
            w_theo, w_resid, w_comb = weights.unbind(-1)  # [B] each
            
            # åˆ›å»ºç»„åˆåˆ†å¸ƒï¼ˆç†è®ºå’Œæ®‹å·®çš„å¹³å‡ï¼‰
            combined_dist = 0.5 * theoretical_dist + 0.5 * residual_corrected_dist
            
            # æœ€ç»ˆèåˆ
            fused_dist = (w_theo.unsqueeze(-1) * theoretical_dist +
                         w_resid.unsqueeze(-1) * residual_corrected_dist +
                         w_comb.unsqueeze(-1) * combined_dist)
        else:
            raise ValueError(f"Unknown fusion strategy: {fusion_strategy}")
        
        # 4. é‡æ–°å½’ä¸€åŒ–
        fused_dist = fused_dist / (torch.sum(fused_dist, dim=1, keepdim=True) + 1e-8)
        
        return {
            'fused_distribution': fused_dist,
            'confidence': confidence,
            'quality_score': quality_score,
            'theoretical_dist': theoretical_dist,
            'residual_corrected_dist': residual_corrected_dist,
            'fusion_weights': weights if fusion_strategy == 'adaptive' else None
        }
    
    def compute_fusion_loss(self,
                          fused_dist: torch.Tensor,
                          target_dist: torch.Tensor,
                          confidence: torch.Tensor,
                          quality_score: torch.Tensor,
                          alpha: float = 0.1,
                          beta: float = 0.05) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—èåˆæŸå¤±
        
        Args:
            fused_dist: èåˆåˆ†å¸ƒ
            target_dist: ç›®æ ‡åˆ†å¸ƒ
            confidence: ç½®ä¿¡åº¦
            quality_score: è´¨é‡è¯„åˆ†
            alpha: ç½®ä¿¡åº¦æ­£åˆ™åŒ–æƒé‡
            beta: è´¨é‡æ­£åˆ™åŒ–æƒé‡
            
        Returns:
            æŸå¤±å­—å…¸
        """
        # ä¸»è¦é‡å»ºæŸå¤±
        recon_loss = F.kl_div(
            torch.log(fused_dist + 1e-8), 
            target_dist, 
            reduction='batchmean'
        )
        
        # ç½®ä¿¡åº¦æ­£åˆ™åŒ–ï¼ˆé¼“åŠ±é€‚åº¦ç½®ä¿¡ï¼‰
        confidence_reg = alpha * torch.mean((confidence - 0.5) ** 2)
        
        # è´¨é‡æ­£åˆ™åŒ–ï¼ˆé¼“åŠ±é«˜è´¨é‡é¢„æµ‹ï¼‰
        quality_reg = beta * torch.mean((1 - quality_score) ** 2)
        
        # æ€»æŸå¤±
        total_loss = recon_loss + confidence_reg + quality_reg
        
        return {
            'total_loss': total_loss,
            'reconstruction_loss': recon_loss,
            'confidence_regularization': confidence_reg,
            'quality_regularization': quality_reg
        }


def test_fusion_module():
    """æµ‹è¯•è‡ªé€‚åº”èåˆæ¨¡å—"""
    print("ğŸ§ª æµ‹è¯•AdaptiveFusionModule...")
    
    # æµ‹è¯•å‚æ•°
    batch_size = 4
    bins = 30
    cond_dim = 64
    
    # åˆ›å»ºæ¨¡å—
    fusion_module = AdaptiveFusionModule(
        cond_dim=cond_dim,
        mayo_param_dim=6,
        hidden_dim=128,
        num_layers=3
    )
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in fusion_module.parameters()):,}")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    theoretical_dist = torch.softmax(torch.randn(batch_size, bins), dim=1)
    residual_corrected_dist = torch.softmax(torch.randn(batch_size, bins), dim=1)
    cond = torch.randn(batch_size, cond_dim)
    
    # æ¨¡æ‹ŸMayo-Lewiså‚æ•°
    mayo_params_batch = [
        {'f_A': 0.4, 'f_B': 0.6, 'p_AA': 0.3, 'p_BB': 0.4, 'p_AB': 0.15, 'p_BA': 0.15},
        {'f_A': 0.5, 'f_B': 0.5, 'p_AA': 0.35, 'p_BB': 0.35, 'p_AB': 0.15, 'p_BA': 0.15},
        {'f_A': 0.6, 'f_B': 0.4, 'p_AA': 0.4, 'p_BB': 0.3, 'p_AB': 0.15, 'p_BA': 0.15},
        {'f_A': 0.3, 'f_B': 0.7, 'p_AA': 0.25, 'p_BB': 0.45, 'p_AB': 0.15, 'p_BA': 0.15}
    ]
    
    # æµ‹è¯•ä¸åŒèåˆç­–ç•¥
    strategies = ['confidence', 'weighted', 'adaptive']
    
    for strategy in strategies:
        print(f"\næµ‹è¯•èåˆç­–ç•¥: {strategy}")
        
        results = fusion_module(
            theoretical_dist=theoretical_dist,
            residual_corrected_dist=residual_corrected_dist,
            cond=cond,
            mayo_params_batch=mayo_params_batch,
            fusion_strategy=strategy
        )
        
        print(f"  èåˆåˆ†å¸ƒå½¢çŠ¶: {results['fused_distribution'].shape}")
        print(f"  ç½®ä¿¡åº¦èŒƒå›´: [{results['confidence'].min():.3f}, {results['confidence'].max():.3f}]")
        print(f"  è´¨é‡è¯„åˆ†èŒƒå›´: [{results['quality_score'].min():.3f}, {results['quality_score'].max():.3f}]")
        print(f"  èåˆåˆ†å¸ƒæ±‚å’Œ: {torch.sum(results['fused_distribution'], dim=1)[:3]}")
        
        if results['fusion_weights'] is not None:
            weights = results['fusion_weights']
            print(f"  æƒé‡åˆ†å¸ƒ: ç†è®º={weights[:, 0].mean():.3f}, "
                  f"æ®‹å·®={weights[:, 1].mean():.3f}, ç»„åˆ={weights[:, 2].mean():.3f}")
    
    # æµ‹è¯•æŸå¤±è®¡ç®—
    print(f"\næµ‹è¯•æŸå¤±è®¡ç®—:")
    target_dist = torch.softmax(torch.randn(batch_size, bins), dim=1)
    
    loss_dict = fusion_module.compute_fusion_loss(
        fused_dist=results['fused_distribution'],
        target_dist=target_dist,
        confidence=results['confidence'],
        quality_score=results['quality_score']
    )
    
    for loss_name, loss_value in loss_dict.items():
        print(f"  {loss_name}: {loss_value.item():.6f}")
    
    print("âœ… AdaptiveFusionModuleæµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    test_fusion_module()
