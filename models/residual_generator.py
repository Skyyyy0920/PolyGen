#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ®‹å·®ç”Ÿæˆå™¨

åŸºäºDiT-1Dæ¶æ„ï¼Œä¸“é—¨å­¦ä¹ å®é™…åˆ†å¸ƒä¸Mayo-Lewisç†è®ºåˆ†å¸ƒçš„åå·®
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Dict


class TimestepEmbedder(nn.Module):
    """æ—¶é—´æ­¥åµŒå…¥å™¨"""
    
    def __init__(self, hidden_size: int, frequency_embedding_size: int = 256):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(frequency_embedding_size, hidden_size, bias=True),
            nn.SiLU(),
            nn.Linear(hidden_size, hidden_size, bias=True),
        )
        self.frequency_embedding_size = frequency_embedding_size

    @staticmethod
    def timestep_embedding(t, dim, max_period=10000):
        """
        åˆ›å»ºæ­£å¼¦æ—¶é—´æ­¥åµŒå…¥
        """
        half = dim // 2
        freqs = torch.exp(
            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half
        ).to(device=t.device)
        args = t[:, None].float() * freqs[None]
        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
        if dim % 2:
            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
        return embedding

    def forward(self, t):
        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)
        t_emb = self.mlp(t_freq)
        return t_emb


class FiLMLayer(nn.Module):
    """Feature-wise Linear Modulationå±‚"""
    
    def __init__(self, cond_dim: int, hidden_size: int):
        super().__init__()
        self.scale_shift_table = nn.Sequential(
            nn.SiLU(),
            nn.Linear(cond_dim, hidden_size * 2, bias=True)
        )
        
    def forward(self, x: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, seq_len, hidden_size]
            cond: æ¡ä»¶ç‰¹å¾ [batch_size, cond_dim]
        """
        scale_shift = self.scale_shift_table(cond)  # [B, hidden_size * 2]
        scale, shift = scale_shift.chunk(2, dim=1)  # [B, hidden_size] each
        
        # æ·»åŠ åºåˆ—ç»´åº¦
        scale = scale.unsqueeze(1)  # [B, 1, hidden_size]
        shift = shift.unsqueeze(1)  # [B, 1, hidden_size]
        
        return x * (1 + scale) + shift


class DiTBlock(nn.Module):
    """
    Diffusion Transformer Block
    
    ç»“åˆè‡ªæ³¨æ„åŠ›å’Œæ¡ä»¶è°ƒåˆ¶çš„Transformerå—
    """
    
    def __init__(self, 
                 hidden_size: int,
                 num_heads: int,
                 cond_dim: int,
                 mlp_ratio: float = 4.0,
                 dropout: float = 0.1,
                 film_each_layer: bool = True):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.film_each_layer = film_each_layer
        
        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        
        # è‡ªæ³¨æ„åŠ›
        self.attn = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # MLP
        mlp_hidden_dim = int(hidden_size * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(hidden_size, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden_dim, hidden_size),
            nn.Dropout(dropout)
        )
        
        # FiLMè°ƒåˆ¶å±‚
        if film_each_layer:
            self.film1 = FiLMLayer(cond_dim, hidden_size)
            self.film2 = FiLMLayer(cond_dim, hidden_size)
        else:
            self.film1 = None
            self.film2 = None
    
    def forward(self, x: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, seq_len, hidden_size]
            cond: æ¡ä»¶ç‰¹å¾ [batch_size, cond_dim]
        """
        # è‡ªæ³¨æ„åŠ›åˆ†æ”¯
        if self.film1 is not None:
            norm1_out = self.film1(self.norm1(x), cond)
        else:
            norm1_out = self.norm1(x)
        
        attn_out, _ = self.attn(norm1_out, norm1_out, norm1_out)
        x = x + attn_out
        
        # MLPåˆ†æ”¯
        if self.film2 is not None:
            norm2_out = self.film2(self.norm2(x), cond)
        else:
            norm2_out = self.norm2(x)
        
        mlp_out = self.mlp(norm2_out)
        x = x + mlp_out
        
        return x


class ResidualGenerator(nn.Module):
    """
    æ®‹å·®ç”Ÿæˆå™¨
    
    åŸºäºDiT-1Dæ¶æ„ï¼Œä¸“é—¨å­¦ä¹ å®é™…åˆ†å¸ƒä¸Mayo-Lewisç†è®ºåˆ†å¸ƒçš„åå·®
    ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¡†æ¶è¿›è¡Œè®­ç»ƒå’Œæ¨ç†
    """
    
    def __init__(self,
                 bins: int = 50,
                 cond_dim: int = 128,
                 hidden_size: int = 256,
                 num_layers: int = 8,
                 num_heads: int = 8,
                 mlp_ratio: float = 4.0,
                 dropout: float = 0.1,
                 film_each_layer: bool = True,
                 learn_sigma: bool = False):
        """
        åˆå§‹åŒ–æ®‹å·®ç”Ÿæˆå™¨
        
        Args:
            bins: åˆ†å¸ƒbinsæ•°é‡
            cond_dim: æ¡ä»¶ç‰¹å¾ç»´åº¦ï¼ˆåŒ…å«ç†è®ºåˆ†å¸ƒï¼‰
            hidden_size: éšè—å±‚å¤§å°
            num_layers: Transformerå±‚æ•°
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            mlp_ratio: MLPæ‰©å±•æ¯”ä¾‹
            dropout: Dropoutç‡
            film_each_layer: æ˜¯å¦æ¯å±‚éƒ½ä½¿ç”¨FiLM
            learn_sigma: æ˜¯å¦å­¦ä¹ æ–¹å·®
        """
        super().__init__()
        
        self.bins = bins
        self.cond_dim = cond_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.learn_sigma = learn_sigma
        
        # æ—¶é—´æ­¥åµŒå…¥
        self.t_embedder = TimestepEmbedder(hidden_size)
        
        # è¾“å…¥æŠ•å½±ï¼ˆå°†æ¯ä¸ªbinæ˜ å°„åˆ°hidden_sizeç»´åº¦ï¼‰
        self.x_embedder = nn.Linear(1, hidden_size)  # æ¯ä¸ªbinå€¼å•ç‹¬åµŒå…¥
        
        # ä½ç½®ç¼–ç ï¼ˆå¯¹äº1Dåˆ†å¸ƒï¼‰
        self.pos_embed = nn.Parameter(torch.zeros(1, bins, hidden_size))
        
        # Transformerå—
        self.blocks = nn.ModuleList([
            DiTBlock(
                hidden_size=hidden_size,
                num_heads=num_heads,
                cond_dim=cond_dim + hidden_size,  # æ¡ä»¶ + æ—¶é—´åµŒå…¥
                mlp_ratio=mlp_ratio,
                dropout=dropout,
                film_each_layer=film_each_layer
            )
            for _ in range(num_layers)
        ])
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        self.final_layer = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        
        # è¾“å‡ºæŠ•å½±ï¼šä»[B, bins, hidden_size]åˆ°[B, bins, 1]æˆ–[B, bins, 2]
        output_channels = 2 if learn_sigma else 1
        self.linear = nn.Linear(hidden_size, output_channels, bias=True)
        
        # åˆå§‹åŒ–æƒé‡
        self.initialize_weights()
    
    def initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        # åˆå§‹åŒ–çº¿æ€§å±‚
        def _basic_init(module):
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
        
        self.apply(_basic_init)
        
        # åˆå§‹åŒ–ä½ç½®ç¼–ç 
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        
        # é›¶åˆå§‹åŒ–æœ€ç»ˆè¾“å‡ºå±‚ï¼ˆé‡è¦ï¼ï¼‰
        nn.init.constant_(self.linear.weight, 0)
        nn.init.constant_(self.linear.bias, 0)
    
    def forward(self, 
                x: torch.Tensor, 
                t: torch.Tensor, 
                cond: torch.Tensor,
                theoretical_dist: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: å™ªå£°è¾“å…¥ [batch_size, bins]
            t: æ—¶é—´æ­¥ [batch_size]
            cond: æ¡ä»¶ç‰¹å¾ [batch_size, cond_dim - bins]
            theoretical_dist: ç†è®ºåˆ†å¸ƒ [batch_size, bins]
            
        Returns:
            é¢„æµ‹çš„æ®‹å·®æˆ–vå‚æ•° [batch_size, bins] æˆ– [batch_size, 2*bins]
        """
        # æ—¶é—´æ­¥åµŒå…¥
        t_emb = self.t_embedder(t)  # [B, hidden_size]
        
        # ç»„åˆæ¡ä»¶ï¼ˆåŸæ¡ä»¶ + ç†è®ºåˆ†å¸ƒï¼‰
        combined_cond = torch.cat([cond, theoretical_dist], dim=1)  # [B, cond_dim]
        
        # ç»„åˆæ¡ä»¶å’Œæ—¶é—´åµŒå…¥
        full_cond = torch.cat([combined_cond, t_emb], dim=1)  # [B, cond_dim + hidden_size]
        
        # è¾“å…¥åµŒå…¥ï¼šå°†[B, bins]è½¬æ¢ä¸º[B, bins, hidden_size]
        x = x.unsqueeze(-1)  # [B, bins, 1]
        x = self.x_embedder(x)  # [B, bins, hidden_size]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed  # [B, bins, hidden_size]
        
        # é€šè¿‡Transformerå—
        for block in self.blocks:
            x = block(x, full_cond)
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        x = self.final_layer(x)
        
        # è¾“å‡ºæŠ•å½±
        x = self.linear(x)  # [B, bins, output_channels]
        
        if self.learn_sigma:
            # åˆ†ç¦»å‡å€¼å’Œæ–¹å·®ï¼š[B, bins, 2] -> [B, bins], [B, bins]
            mean, logvar = x.chunk(2, dim=-1)  # [B, bins, 1] each
            return mean.squeeze(-1), logvar.squeeze(-1)  # [B, bins] each
        else:
            return x.squeeze(-1)  # [B, bins]
    
    def get_condition_dim(self) -> int:
        """è·å–å®Œæ•´æ¡ä»¶ç»´åº¦ï¼ˆä¸åŒ…æ‹¬ç†è®ºåˆ†å¸ƒï¼‰"""
        return self.cond_dim - self.bins


class NoiseSchedule:
    """æ‰©æ•£å™ªå£°è°ƒåº¦å™¨"""
    
    def __init__(self, 
                 T: int = 1000,
                 beta_start: float = 1e-4,
                 beta_end: float = 0.02,
                 schedule_type: str = 'cosine'):
        """
        åˆå§‹åŒ–å™ªå£°è°ƒåº¦å™¨
        
        Args:
            T: æ‰©æ•£æ­¥æ•°
            beta_start: èµ·å§‹betaå€¼
            beta_end: ç»“æŸbetaå€¼
            schedule_type: è°ƒåº¦ç±»å‹ ('linear' or 'cosine')
        """
        self.T = T
        
        if schedule_type == 'linear':
            self.betas = torch.linspace(beta_start, beta_end, T)
        elif schedule_type == 'cosine':
            # ä½™å¼¦è°ƒåº¦
            steps = T + 1
            x = torch.linspace(0, T, steps)
            alphas_cumprod = torch.cos(((x / T) + 0.008) / 1.008 * math.pi * 0.5) ** 2
            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
            betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
            self.betas = torch.clamp(betas, 0.0001, 0.9999)
        else:
            raise ValueError(f"Unknown schedule type: {schedule_type}")
        
        self.alphas = 1 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)
        
        # v-å‚æ•°åŒ–æ‰€éœ€çš„ç³»æ•°
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)
    
    def to(self, device):
        """ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        for attr in ['betas', 'alphas', 'alphas_cumprod', 'alphas_cumprod_prev',
                     'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod']:
            setattr(self, attr, getattr(self, attr).to(device))
        return self


def q_sample_vparam(x_start: torch.Tensor, 
                   t: torch.Tensor, 
                   schedule: NoiseSchedule,
                   noise: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    v-å‚æ•°åŒ–çš„å‰å‘æ‰©æ•£è¿‡ç¨‹
    
    Args:
        x_start: åŸå§‹æ•°æ® [B, ...]
        t: æ—¶é—´æ­¥ [B]
        schedule: å™ªå£°è°ƒåº¦å™¨
        noise: å¯é€‰çš„å™ªå£°å¼ é‡
        
    Returns:
        x_t: åŠ å™ªåçš„æ•°æ®
        v_target: vå‚æ•°åŒ–çš„ç›®æ ‡
    """
    if noise is None:
        noise = torch.randn_like(x_start)
    
    device = x_start.device
    schedule = schedule.to(device)

    # è·å–æ—¶é—´æ­¥å¯¹åº”çš„ç³»æ•°
    sqrt_alphas_cumprod_t = schedule.sqrt_alphas_cumprod[t].view(-1, *([1] * (x_start.ndim - 1)))
    sqrt_one_minus_alphas_cumprod_t = schedule.sqrt_one_minus_alphas_cumprod[t].view(-1, *([1] * (x_start.ndim - 1)))
    
    # å‰å‘è¿‡ç¨‹
    x_t = sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise
    
    # v-å‚æ•°åŒ–ç›®æ ‡
    v_target = sqrt_alphas_cumprod_t * noise - sqrt_one_minus_alphas_cumprod_t * x_start
    
    return x_t, v_target


def test_residual_generator():
    """æµ‹è¯•æ®‹å·®ç”Ÿæˆå™¨"""
    print("ğŸ§ª æµ‹è¯•ResidualGenerator...")
    
    # æµ‹è¯•å‚æ•°
    batch_size = 4
    bins = 30
    cond_dim = 64  # ä¸åŒ…æ‹¬ç†è®ºåˆ†å¸ƒ
    
    # åˆ›å»ºæ¨¡å‹
    model = ResidualGenerator(
        bins=bins,
        cond_dim=cond_dim + bins,  # åŒ…æ‹¬ç†è®ºåˆ†å¸ƒ
        hidden_size=128,
        num_layers=4,
        num_heads=8
    )
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, bins)
    t = torch.randint(0, 1000, (batch_size,))
    cond = torch.randn(batch_size, cond_dim)
    theoretical_dist = torch.softmax(torch.randn(batch_size, bins), dim=1)
    
    # å‰å‘ä¼ æ’­
    output = model(x, t, cond, theoretical_dist)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"æ—¶é—´æ­¥å½¢çŠ¶: {t.shape}")
    print(f"æ¡ä»¶å½¢çŠ¶: {cond.shape}")
    print(f"ç†è®ºåˆ†å¸ƒå½¢çŠ¶: {theoretical_dist.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # æµ‹è¯•å™ªå£°è°ƒåº¦å™¨
    print("\nğŸ§ª æµ‹è¯•NoiseSchedule...")
    
    schedule = NoiseSchedule(T=100, schedule_type='cosine')
    
    # æµ‹è¯•v-å‚æ•°åŒ–é‡‡æ ·
    x_start = torch.softmax(torch.randn(batch_size, bins), dim=1)
    t_sample = torch.randint(0, 100, (batch_size,))
    
    x_t, v_target = q_sample_vparam(x_start, t_sample, schedule)
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {x_start.shape}")
    print(f"åŠ å™ªæ•°æ®å½¢çŠ¶: {x_t.shape}")
    print(f"vç›®æ ‡å½¢çŠ¶: {v_target.shape}")
    print(f"åŸå§‹æ•°æ®æ±‚å’Œ: {torch.sum(x_start, dim=1)[:3]}")
    print(f"åŠ å™ªæ•°æ®æ±‚å’Œ: {torch.sum(x_t, dim=1)[:3]}")
    
    # æµ‹è¯•æŸå¤±è®¡ç®—
    v_pred = model(x_t, t_sample, cond, theoretical_dist)
    loss = F.mse_loss(v_pred, v_target)
    
    print(f"vé¢„æµ‹å½¢çŠ¶: {v_pred.shape}")
    print(f"MSEæŸå¤±: {loss.item():.6f}")
    
    print("âœ… ResidualGeneratoræµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    test_residual_generator()
