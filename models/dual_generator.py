#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒç”Ÿæˆå™¨ä¸»æ¨¡å‹

æ•´åˆMayo-Lewisç†è®ºè®¡ç®—ã€æ®‹å·®ç”Ÿæˆå™¨å’Œè‡ªé€‚åº”èåˆæ¨¡å—
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple

from ..data.mayo_lewis import MayoLewisCalculator
from .condition_encoder import ConditionEncoder, SequenceEncoder
from .residual_generator import ResidualGenerator, NoiseSchedule, q_sample_vparam
from .fusion_module import AdaptiveFusionModule


class DualGeneratorModel(nn.Module):
    """
    åŒç”Ÿæˆå™¨èšåˆç‰©å—åˆ†å¸ƒç”Ÿæˆæ¨¡å‹
    
    æ¶æ„ç»„æˆï¼š
    1. Mayo-Lewisç†è®ºè®¡ç®—å™¨ï¼ˆç¡®å®šæ€§ï¼Œæ— å‚æ•°ï¼‰
    2. æ¡ä»¶ç¼–ç å™¨ï¼ˆç¼–ç å®éªŒæ¡ä»¶å’Œåºåˆ—ä¿¡æ¯ï¼‰
    3. æ®‹å·®ç”Ÿæˆå™¨ï¼ˆåŸºäºæ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ ç†è®ºåå·®ï¼‰
    4. è‡ªé€‚åº”èåˆæ¨¡å—ï¼ˆåŠ¨æ€ç»„åˆç†è®ºå’Œæ®‹å·®åˆ†å¸ƒï¼‰
    """
    
    def __init__(self,
                 bins: int = 50,
                 condition_dim: int = 17,

                 cond_encoder_d_model: int = 128,
                 cond_encoder_proj_dim: int = 256,
                 cond_encoder_layers: int = 3,

                 seq_encoder_d_model: int = 64,
                 seq_encoder_layers: int = 2,

                 residual_hidden_size: int = 256,
                 residual_num_layers: int = 8,
                 residual_num_heads: int = 8,

                 fusion_hidden_dim: int = 128,
                 fusion_num_layers: int = 3,

                 diffusion_steps: int = 1000,
                 noise_schedule: str = 'cosine',

                 dropout: float = 0.1,
                 temperature: float = 0.1):
        super().__init__()
        
        self.bins = bins
        self.condition_dim = condition_dim
        self.diffusion_steps = diffusion_steps
        
        # 1. Mayo-Lewisç†è®ºè®¡ç®—å™¨ï¼ˆæ— å‚æ•°ï¼‰
        self.mayo_lewis_calc = MayoLewisCalculator(max_length=bins)
        
        # 2. æ¡ä»¶ç¼–ç å™¨
        self.condition_encoder = ConditionEncoder(
            in_dim=condition_dim,
            d_model=cond_encoder_d_model,
            proj_dim=cond_encoder_proj_dim,
            num_layers=cond_encoder_layers,
            dropout=dropout,
            temperature=temperature
        )
        
        # 3. åºåˆ—ç¼–ç å™¨
        self.sequence_encoder = SequenceEncoder(
            vocab_size=3,  # 0: pad, 1: A, 2: B
            d_model=seq_encoder_d_model,
            num_layers=seq_encoder_layers,
            max_length=500,  # è®¾ç½®åˆç†çš„æœ€å¤§åºåˆ—é•¿åº¦
            dropout=dropout
        )
        
        # 4. æ®‹å·®ç”Ÿæˆå™¨
        residual_cond_dim = (cond_encoder_d_model + seq_encoder_d_model + bins)
        self.residual_generator = ResidualGenerator(
            bins=bins,
            cond_dim=residual_cond_dim,
            hidden_size=residual_hidden_size,
            num_layers=residual_num_layers,
            num_heads=residual_num_heads,
            dropout=dropout
        )
        
        # 5. è‡ªé€‚åº”èåˆæ¨¡å—
        fusion_cond_dim = cond_encoder_d_model + seq_encoder_d_model
        self.fusion_module = AdaptiveFusionModule(
            cond_dim=fusion_cond_dim,
            mayo_param_dim=6,
            hidden_dim=fusion_hidden_dim,
            num_layers=fusion_num_layers,
            dropout=dropout
        )
        
        # 6. æ‰©æ•£å™ªå£°è°ƒåº¦å™¨
        self.noise_schedule = NoiseSchedule(
            T=diffusion_steps,
            schedule_type=noise_schedule
        )
        
        print(f"ğŸ—ï¸ DualGeneratorModelåˆå§‹åŒ–å®Œæˆ:")
        print(f"  æ€»å‚æ•°æ•°é‡: {self.count_parameters():,}")
        print(f"  åˆ†å¸ƒbins: {bins}")
        print(f"  æ‰©æ•£æ­¥æ•°: {diffusion_steps}")
        print(f"  å™ªå£°è°ƒåº¦: {noise_schedule}")
    
    def count_parameters(self) -> int:
        """è®¡ç®—æ¨¡å‹æ€»å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def get_theoretical_distributions(self, sequences_batch: List[List[str]]) -> Tuple[torch.Tensor, List[Dict]]:
        """
        è·å–Mayo-Lewisç†è®ºåˆ†å¸ƒ
        
        Args:
            sequences_batch: æ‰¹é‡åºåˆ—æ•°æ®
            
        Returns:
            theoretical_dists: ç†è®ºåˆ†å¸ƒ [batch_size, bins]
            mayo_params: Mayo-Lewiså‚æ•°åˆ—è¡¨
        """
        theoretical_dists = self.mayo_lewis_calc.batch_calculate_distributions(sequences_batch)
        
        mayo_params = []
        for sequences in sequences_batch:
            params = self.mayo_lewis_calc.extract_sequence_statistics(sequences)
            mayo_params.append(params)
        
        return theoretical_dists, mayo_params
    
    def encode_conditions(self, 
                         condition_features: torch.Tensor,
                         sequences_batch: List[List[str]]) -> Dict[str, torch.Tensor]:
        """
        ç¼–ç æ¡ä»¶ç‰¹å¾å’Œåºåˆ—ä¿¡æ¯
        
        Args:
            condition_features: æ¡ä»¶ç‰¹å¾ [batch_size, condition_dim]
            sequences_batch: åºåˆ—æ‰¹æ¬¡
            
        Returns:
            ç¼–ç ç»“æœå­—å…¸
        """
        # æ¡ä»¶ç¼–ç 
        cond_results = self.condition_encoder(condition_features)
        
        # åºåˆ—ç¼–ç 
        seq_results = self.sequence_encoder(sequences_batch)
        
        # ç»„åˆç¼–ç 
        combined_embedding = torch.cat([
            cond_results['cond_emb'],
            seq_results['sequence_embedding']
        ], dim=1)
        
        return {
            'condition_embedding': cond_results['cond_emb'],
            'sequence_embedding': seq_results['sequence_embedding'],
            'combined_embedding': combined_embedding,
            'projection_embedding': cond_results['proj_emb']
        }
    
    def forward_residual_generator(self,
                                 x: torch.Tensor,
                                 t: torch.Tensor,
                                 combined_embedding: torch.Tensor,
                                 theoretical_dist: torch.Tensor) -> torch.Tensor:
        """
        æ®‹å·®ç”Ÿæˆå™¨å‰å‘ä¼ æ’­
        
        Args:
            x: å™ªå£°è¾“å…¥ [batch_size, bins]
            t: æ—¶é—´æ­¥ [batch_size]
            combined_embedding: ç»„åˆåµŒå…¥ [batch_size, combined_dim]
            theoretical_dist: ç†è®ºåˆ†å¸ƒ [batch_size, bins]
            
        Returns:
            æ®‹å·®é¢„æµ‹ [batch_size, bins]
        """
        return self.residual_generator(x, t, combined_embedding, theoretical_dist)
    
    def compute_residual_loss(self,
                            residual_targets: torch.Tensor,
                            condition_features: torch.Tensor,
                            sequences_batch: List[List[str]],
                            theoretical_dists: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æ®‹å·®ç”Ÿæˆå™¨æŸå¤±
        
        Args:
            residual_targets: æ®‹å·®ç›®æ ‡ [batch_size, bins]
            condition_features: æ¡ä»¶ç‰¹å¾
            sequences_batch: åºåˆ—æ‰¹æ¬¡
            theoretical_dists: ç†è®ºåˆ†å¸ƒ
            
        Returns:
            æŸå¤±å­—å…¸
        """
        device = residual_targets.device
        batch_size = residual_targets.size(0)
        
        # ç¼–ç æ¡ä»¶
        encoding_results = self.encode_conditions(condition_features, sequences_batch)
        combined_embedding = encoding_results['combined_embedding']
        
        # éšæœºæ—¶é—´æ­¥
        t = torch.randint(0, self.diffusion_steps, (batch_size,), device=device)
        
        # v-å‚æ•°åŒ–æ‰©æ•£é‡‡æ ·
        x_t, v_target = q_sample_vparam(residual_targets, t, self.noise_schedule.to(device))
        
        # æ®‹å·®ç”Ÿæˆå™¨é¢„æµ‹
        v_pred = self.forward_residual_generator(x_t, t, combined_embedding, theoretical_dists)
        
        # MSEæŸå¤±
        residual_loss = F.mse_loss(v_pred, v_target)
        
        return {
            'residual_loss': residual_loss,
            'v_pred': v_pred,
            'v_target': v_target,
            'encoding_results': encoding_results
        }
    
    def sample_residual_distribution(self,
                                   condition_features: torch.Tensor,
                                   sequences_batch: List[List[str]],
                                   theoretical_dists: torch.Tensor,
                                   num_steps: int = 50,
                                   guidance_scale: float = 1.0,
                                   temperature: float = 1.0) -> torch.Tensor:
        """
        é‡‡æ ·æ®‹å·®åˆ†å¸ƒ
        
        Args:
            condition_features: æ¡ä»¶ç‰¹å¾
            sequences_batch: åºåˆ—æ‰¹æ¬¡
            theoretical_dists: ç†è®ºåˆ†å¸ƒ
            num_steps: DDIMé‡‡æ ·æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            temperature: é‡‡æ ·æ¸©åº¦
            
        Returns:
            é‡‡æ ·çš„æ®‹å·®åˆ†å¸ƒ [batch_size, bins]
        """
        device = condition_features.device
        batch_size = condition_features.size(0)
        
        # ç¼–ç æ¡ä»¶
        encoding_results = self.encode_conditions(condition_features, sequences_batch)
        combined_embedding = encoding_results['combined_embedding']
        
        # DDIMé‡‡æ ·ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        schedule = self.noise_schedule.to(device)
        
        # åˆå§‹å™ªå£°
        x = torch.randn(batch_size, self.bins, device=device)
        
        # é‡‡æ ·æ­¥éª¤
        timesteps = torch.linspace(self.diffusion_steps - 1, 0, num_steps, dtype=torch.long, device=device)
        
        for i, t in enumerate(timesteps):
            t_batch = t.expand(batch_size)
            
            # é¢„æµ‹v
            with torch.no_grad():
                v_pred = self.forward_residual_generator(x, t_batch, combined_embedding, theoretical_dists)
            
            # DDIMæ›´æ–°æ­¥éª¤ï¼ˆç®€åŒ–ï¼‰
            alpha_t = schedule.alphas_cumprod[t]
            alpha_prev = schedule.alphas_cumprod[timesteps[i + 1]] if i < len(timesteps) - 1 else torch.tensor(1.0)
            
            # è®¡ç®—x0é¢„æµ‹
            sqrt_alpha_t = torch.sqrt(alpha_t)
            sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)
            
            x0_pred = sqrt_alpha_t * x + sqrt_one_minus_alpha_t * v_pred
            
            if i < len(timesteps) - 1:
                # è®¡ç®—ä¸‹ä¸€æ­¥
                sqrt_alpha_prev = torch.sqrt(alpha_prev)
                sqrt_one_minus_alpha_prev = torch.sqrt(1 - alpha_prev)
                
                x = sqrt_alpha_prev * x0_pred + sqrt_one_minus_alpha_prev * v_pred
            else:
                x = x0_pred
        
        return x
    
    def forward(self,
                condition_features: torch.Tensor,
                sequences_batch: List[List[str]],
                mode: str = 'inference',
                **kwargs) -> Dict[str, torch.Tensor]:
        """
        ä¸»å‰å‘ä¼ æ’­å‡½æ•°
        
        Args:
            condition_features: æ¡ä»¶ç‰¹å¾ [batch_size, condition_dim]
            sequences_batch: åºåˆ—æ‰¹æ¬¡
            mode: æ¨¡å¼ ('training', 'inference')
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç»“æœå­—å…¸
        """
        device = condition_features.device
        
        # 1. è·å–Mayo-Lewisç†è®ºåˆ†å¸ƒ
        theoretical_dists, mayo_params = self.get_theoretical_distributions(sequences_batch)
        theoretical_dists = theoretical_dists.to(device)
        
        if mode == 'training':
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—æŸå¤±
            residual_targets = kwargs.get('residual_targets')
            if residual_targets is None:
                raise ValueError("Training mode requires residual_targets")
            
            loss_results = self.compute_residual_loss(
                residual_targets, condition_features, sequences_batch, theoretical_dists
            )
            
            return {
                'theoretical_distributions': theoretical_dists,
                'mayo_parameters': mayo_params,
                **loss_results
            }
        
        elif mode == 'inference':
            # æ¨ç†æ¨¡å¼ï¼šç”Ÿæˆåˆ†å¸ƒ
            
            # 2. é‡‡æ ·æ®‹å·®åˆ†å¸ƒ
            residual_dist = self.sample_residual_distribution(
                condition_features, sequences_batch, theoretical_dists,
                num_steps=kwargs.get('num_steps', 50),
                guidance_scale=kwargs.get('guidance_scale', 1.0),
                temperature=kwargs.get('temperature', 1.0)
            )
            
            # 3. ç»„åˆåˆ†å¸ƒï¼ˆç†è®º + æ®‹å·®ï¼‰
            residual_corrected_dist = theoretical_dists + residual_dist
            
            # ç¡®ä¿éè´Ÿå¹¶å½’ä¸€åŒ–
            residual_corrected_dist = F.relu(residual_corrected_dist)
            residual_corrected_dist = residual_corrected_dist / (
                torch.sum(residual_corrected_dist, dim=1, keepdim=True) + 1e-8
            )
            
            # 4. ç¼–ç æ¡ä»¶ç”¨äºèåˆ
            encoding_results = self.encode_conditions(condition_features, sequences_batch)
            
            # 5. è‡ªé€‚åº”èåˆ
            fusion_results = self.fusion_module(
                theoretical_dist=theoretical_dists,
                residual_corrected_dist=residual_corrected_dist,
                cond=encoding_results['combined_embedding'],
                mayo_params_batch=mayo_params,
                fusion_strategy=kwargs.get('fusion_strategy', 'adaptive')
            )
            
            return {
                'theoretical_distributions': theoretical_dists,
                'residual_distributions': residual_dist,
                'residual_corrected_distributions': residual_corrected_dist,
                'mayo_parameters': mayo_params,
                'encoding_results': encoding_results,
                **fusion_results
            }
        
        else:
            raise ValueError(f"Unknown mode: {mode}")
    
    def get_model_info(self) -> Dict[str, any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_name': 'DualGeneratorModel',
            'total_parameters': self.count_parameters(),
            'bins': self.bins,
            'condition_dim': self.condition_dim,
            'diffusion_steps': self.diffusion_steps,
            'components': {
                'mayo_lewis_calculator': 'MayoLewisCalculator (no parameters)',
                'condition_encoder': f'{sum(p.numel() for p in self.condition_encoder.parameters()):,} params',
                'sequence_encoder': f'{sum(p.numel() for p in self.sequence_encoder.parameters()):,} params',
                'residual_generator': f'{sum(p.numel() for p in self.residual_generator.parameters()):,} params',
                'fusion_module': f'{sum(p.numel() for p in self.fusion_module.parameters()):,} params'
            }
        }


def test_dual_generator_model():
    """æµ‹è¯•åŒç”Ÿæˆå™¨æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•DualGeneratorModel...")
    
    # æµ‹è¯•å‚æ•°
    batch_size = 2  # å‡å°æ‰¹æ¬¡å¤§å°ä»¥èŠ‚çœå†…å­˜
    bins = 30
    condition_dim = 17
    
    # åˆ›å»ºæ¨¡å‹
    model = DualGeneratorModel(
        bins=bins,
        condition_dim=condition_dim,
        cond_encoder_d_model=64,  # å‡å°æ¨¡å‹å¤§å°
        residual_hidden_size=128,
        residual_num_layers=4,
        fusion_hidden_dim=64,
        diffusion_steps=100  # å‡å°‘æ‰©æ•£æ­¥æ•°
    )
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    model_info = model.get_model_info()
    print(f"æ¨¡å‹ä¿¡æ¯:")
    for key, value in model_info.items():
        if key == 'components':
            print(f"  {key}:")
            for comp_name, comp_info in value.items():
                print(f"    {comp_name}: {comp_info}")
        else:
            print(f"  {key}: {value}")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    condition_features = torch.randn(batch_size, condition_dim)
    sequences_batch = [
        ['AAABBBAAABBB', 'BBAABBAA'],
        ['ABABABABAB', 'BABABA']
    ]
    
    # æµ‹è¯•æ¨ç†æ¨¡å¼
    print(f"\næµ‹è¯•æ¨ç†æ¨¡å¼:")
    with torch.no_grad():
        results = model(
            condition_features=condition_features,
            sequences_batch=sequences_batch,
            mode='inference',
            num_steps=10,  # å‡å°‘é‡‡æ ·æ­¥æ•°
            fusion_strategy='adaptive'
        )
    
    print(f"  ç†è®ºåˆ†å¸ƒå½¢çŠ¶: {results['theoretical_distributions'].shape}")
    print(f"  æ®‹å·®åˆ†å¸ƒå½¢çŠ¶: {results['residual_distributions'].shape}")
    print(f"  èåˆåˆ†å¸ƒå½¢çŠ¶: {results['fused_distribution'].shape}")
    print(f"  ç½®ä¿¡åº¦: {results['confidence']}")
    print(f"  è´¨é‡è¯„åˆ†: {results['quality_score']}")
    
    # æ£€æŸ¥åˆ†å¸ƒå½’ä¸€åŒ–
    fused_sums = torch.sum(results['fused_distribution'], dim=1)
    print(f"  èåˆåˆ†å¸ƒæ±‚å’Œ: {fused_sums}")
    
    # æµ‹è¯•è®­ç»ƒæ¨¡å¼
    print(f"\næµ‹è¯•è®­ç»ƒæ¨¡å¼:")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ®‹å·®ç›®æ ‡
    residual_targets = torch.randn(batch_size, bins) * 0.1  # å°çš„æ®‹å·®
    
    train_results = model(
        condition_features=condition_features,
        sequences_batch=sequences_batch,
        mode='training',
        residual_targets=residual_targets
    )
    
    print(f"  æ®‹å·®æŸå¤±: {train_results['residual_loss'].item():.6f}")
    print(f"  vé¢„æµ‹å½¢çŠ¶: {train_results['v_pred'].shape}")
    print(f"  vç›®æ ‡å½¢çŠ¶: {train_results['v_target'].shape}")
    
    print("âœ… DualGeneratorModelæµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    test_dual_generator_model()
