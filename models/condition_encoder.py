#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¡ä»¶ç¼–ç å™¨

åŸºäºPolyGen-F06Cçš„ConditionEncoderï¼Œé€‚é…åŒç”Ÿæˆå™¨éœ€æ±‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional


class ConditionEncoder(nn.Module):
    """
    æ¡ä»¶ç¼–ç å™¨
    
    å°†èšåˆç‰©æ¡ä»¶ç‰¹å¾ç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤ºï¼Œç”¨äºæŒ‡å¯¼æ®‹å·®ç”Ÿæˆå™¨
    """
    
    def __init__(self,
                 in_dim: int = 17,
                 d_model: int = 128,
                 proj_dim: int = 256,
                 num_layers: int = 3,
                 dropout: float = 0.1,
                 temperature: float = 0.10):
        """
        åˆå§‹åŒ–æ¡ä»¶ç¼–ç å™¨
        
        Args:
            in_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            d_model: æ¨¡å‹éšè—ç»´åº¦
            proj_dim: æŠ•å½±ç»´åº¦
            num_layers: Transformerå±‚æ•°
            dropout: Dropoutç‡
            temperature: å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°
        """
        super().__init__()
        
        self.in_dim = in_dim
        self.d_model = d_model
        self.proj_dim = proj_dim
        self.temperature = temperature
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Sequential(
            nn.Linear(in_dim, d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # Transformerç¼–ç å™¨å±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=8,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )
        
        # è¾“å‡ºæŠ•å½±å¤´
        self.projection_head = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, proj_dim)
        )
        
        # æ¡ä»¶åµŒå…¥è¾“å‡º
        self.condition_head = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_model)
        )
        
    def forward(self, cond: torch.Tensor, return_features: bool = False) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            cond: æ¡ä»¶ç‰¹å¾ [batch_size, in_dim]
            return_features: æ˜¯å¦è¿”å›ä¸­é—´ç‰¹å¾
            
        Returns:
            åŒ…å«å„ç§åµŒå…¥çš„å­—å…¸
        """
        batch_size = cond.size(0)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(cond)  # [B, d_model]
        
        # æ·»åŠ åºåˆ—ç»´åº¦ç”¨äºTransformer
        x = x.unsqueeze(1)  # [B, 1, d_model]
        
        # Transformerç¼–ç 
        encoded = self.transformer_encoder(x)  # [B, 1, d_model]
        encoded = encoded.squeeze(1)  # [B, d_model]
        
        # ç”Ÿæˆä¸åŒçš„åµŒå…¥
        proj_emb = self.projection_head(encoded)  # [B, proj_dim] - ç”¨äºå¯¹æ¯”å­¦ä¹ 
        cond_emb = self.condition_head(encoded)   # [B, d_model] - ç”¨äºæ¡ä»¶ç”Ÿæˆ
        
        # L2å½’ä¸€åŒ–æŠ•å½±åµŒå…¥
        proj_emb = F.normalize(proj_emb, p=2, dim=1)
        
        results = {
            'proj_emb': proj_emb,      # å¯¹æ¯”å­¦ä¹ åµŒå…¥
            'cond_emb': cond_emb,      # æ¡ä»¶ç”ŸæˆåµŒå…¥
        }
        
        if return_features:
            results.update({
                'input_features': cond,
                'projected_features': x.squeeze(1),
                'encoded_features': encoded
            })
        
        return results
    
    def compute_contrastive_loss(self, 
                                proj_emb1: torch.Tensor, 
                                proj_emb2: torch.Tensor,
                                labels: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±
        
        Args:
            proj_emb1: ç¬¬ä¸€ç»„æŠ•å½±åµŒå…¥ [B, proj_dim]
            proj_emb2: ç¬¬äºŒç»„æŠ•å½±åµŒå…¥ [B, proj_dim]
            labels: å¯é€‰çš„æ ‡ç­¾ï¼Œç”¨äºç›‘ç£å¯¹æ¯”å­¦ä¹ 
            
        Returns:
            å¯¹æ¯”æŸå¤±
        """
        batch_size = proj_emb1.size(0)
        device = proj_emb1.device
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(proj_emb1, proj_emb2.T) / self.temperature
        
        if labels is None:
            # æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼šå¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬
            labels = torch.arange(batch_size, device=device)
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss = F.cross_entropy(similarity_matrix, labels)
        
        return loss
    
    def get_embedding_dim(self) -> int:
        """è·å–æ¡ä»¶åµŒå…¥ç»´åº¦"""
        return self.d_model


class SequenceEncoder(nn.Module):
    """
    åºåˆ—ç¼–ç å™¨
    
    ä¸“é—¨ç”¨äºç¼–ç èšåˆç‰©åºåˆ—ï¼Œç”Ÿæˆåºåˆ—çº§åˆ«çš„è¡¨ç¤º
    """
    
    def __init__(self,
                 vocab_size: int = 3,  # 0: pad, 1: A, 2: B
                 d_model: int = 128,
                 num_layers: int = 4,
                 num_heads: int = 8,
                 max_length: int = 1000,
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–åºåˆ—ç¼–ç å™¨
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            d_model: æ¨¡å‹ç»´åº¦
            num_layers: Transformerå±‚æ•°
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            dropout: Dropoutç‡
        """
        super().__init__()
        
        self.d_model = d_model
        self.max_length = max_length
        
        # TokenåµŒå…¥
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.position_encoding = nn.Parameter(
            torch.randn(1, max_length, d_model) * 0.02
        )
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )
        
        # æ± åŒ–å±‚
        self.pooling = nn.AdaptiveAvgPool1d(1)
        
        self.dropout = nn.Dropout(dropout)
        
    def tokenize_sequences(self, sequences_batch: list) -> torch.Tensor:
        """
        å°†åºåˆ—æ‰¹æ¬¡è½¬æ¢ä¸ºtokenå¼ é‡
        
        Args:
            sequences_batch: åºåˆ—æ‰¹æ¬¡ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯åºåˆ—åˆ—è¡¨
            
        Returns:
            Tokenå¼ é‡ [batch_size, max_seq_len]
        """
        batch_tokens = []
        
        for sequences in sequences_batch:
            # åˆå¹¶åºåˆ—
            combined_seq = ''.join(sequences)[:self.max_length]
            
            # è½¬æ¢ä¸ºtoken
            tokens = []
            for char in combined_seq:
                if char == 'A':
                    tokens.append(1)
                elif char == 'B':
                    tokens.append(2)
                else:
                    tokens.append(0)  # padding
            
            # å¡«å……åˆ°æœ€å¤§é•¿åº¦
            while len(tokens) < self.max_length:
                tokens.append(0)
            
            batch_tokens.append(tokens)
        
        return torch.tensor(batch_tokens, dtype=torch.long)
    
    def forward(self, sequences_batch: list) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            sequences_batch: åºåˆ—æ‰¹æ¬¡
            
        Returns:
            ç¼–ç ç»“æœå­—å…¸
        """
        # TokenåŒ–
        tokens = self.tokenize_sequences(sequences_batch)  # [B, L]
        device = next(self.parameters()).device
        tokens = tokens.to(device)
        
        batch_size, seq_len = tokens.shape
        
        # TokenåµŒå…¥
        x = self.token_embedding(tokens)  # [B, L, d_model]
        
        # ä½ç½®ç¼–ç 
        if seq_len <= self.max_length:
            pos_enc = self.position_encoding[:, :seq_len, :]
        else:
            # å¤„ç†è¶…é•¿åºåˆ—
            pos_enc = self.position_encoding.repeat(1, (seq_len // self.max_length) + 1, 1)[:, :seq_len, :]
        
        x = x + pos_enc
        x = self.dropout(x)
        
        # åˆ›å»ºpadding mask
        padding_mask = (tokens == 0)  # [B, L]
        
        # Transformerç¼–ç 
        encoded = self.transformer(x, src_key_padding_mask=padding_mask)  # [B, L, d_model]
        
        # æ± åŒ–å¾—åˆ°åºåˆ—çº§è¡¨ç¤º
        # åªå¯¹épaddingä½ç½®è¿›è¡Œæ± åŒ–
        mask = (~padding_mask).float().unsqueeze(-1)  # [B, L, 1]
        masked_encoded = encoded * mask
        
        # å¹³å‡æ± åŒ–
        seq_lengths = mask.sum(dim=1, keepdim=True)  # [B, 1, 1]
        seq_repr = masked_encoded.sum(dim=1) / (seq_lengths.squeeze(-1) + 1e-8)  # [B, d_model]
        
        return {
            'sequence_embedding': seq_repr,
            'token_embeddings': encoded,
            'attention_mask': ~padding_mask
        }


def test_condition_encoder():
    """æµ‹è¯•æ¡ä»¶ç¼–ç å™¨"""
    print("ğŸ§ª æµ‹è¯•ConditionEncoder...")
    
    # æµ‹è¯•å‚æ•°
    batch_size = 4
    in_dim = 17
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = ConditionEncoder(
        in_dim=in_dim,
        d_model=128,
        proj_dim=256,
        num_layers=3
    )
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    cond_features = torch.randn(batch_size, in_dim)
    
    # å‰å‘ä¼ æ’­
    results = encoder(cond_features, return_features=True)
    
    print(f"è¾“å…¥å½¢çŠ¶: {cond_features.shape}")
    print(f"æŠ•å½±åµŒå…¥å½¢çŠ¶: {results['proj_emb'].shape}")
    print(f"æ¡ä»¶åµŒå…¥å½¢çŠ¶: {results['cond_emb'].shape}")
    
    # æµ‹è¯•å¯¹æ¯”å­¦ä¹ æŸå¤±
    proj_emb1 = results['proj_emb']
    proj_emb2 = torch.randn_like(proj_emb1)
    proj_emb2 = F.normalize(proj_emb2, p=2, dim=1)
    
    contrastive_loss = encoder.compute_contrastive_loss(proj_emb1, proj_emb2)
    print(f"å¯¹æ¯”æŸå¤±: {contrastive_loss.item():.4f}")
    
    print("âœ… ConditionEncoderæµ‹è¯•é€šè¿‡!")
    
    # æµ‹è¯•åºåˆ—ç¼–ç å™¨
    print("\nğŸ§ª æµ‹è¯•SequenceEncoder...")
    
    seq_encoder = SequenceEncoder(d_model=64, num_layers=2)
    
    # æµ‹è¯•åºåˆ—
    sequences_batch = [
        ['AAABBB', 'BBAABB'],
        ['ABABAB', 'BABABA'],
        ['AAAAAA', 'BBBBBB'],
        ['ABABABAB']
    ]
    
    seq_results = seq_encoder(sequences_batch)
    
    print(f"åºåˆ—åµŒå…¥å½¢çŠ¶: {seq_results['sequence_embedding'].shape}")
    print(f"TokenåµŒå…¥å½¢çŠ¶: {seq_results['token_embeddings'].shape}")
    
    print("âœ… SequenceEncoderæµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    test_condition_encoder()
