#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒç”Ÿæˆå™¨è®­ç»ƒå™¨

å®ç°å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬æ¡ä»¶ç¼–ç å™¨é¢„è®­ç»ƒå’ŒåŒç”Ÿæˆå™¨è”åˆè®­ç»ƒ
"""

import os
import time
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

from ..models.dual_generator import DualGeneratorModel
from ..data.dataset import DualPolyDataset, collate_dual_poly
from .utils import EvaluationMetrics


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # åŸºæœ¬é…ç½®
    experiment_name: str = "dual_generator_experiment"
    output_dir: str = "outputs"
    device: str = "auto"  # "auto", "cpu", "cuda"
    
    # æ•°æ®é…ç½®
    csv_path: str = "PolyGen-F06C/data/copolymer.csv"
    max_samples: Optional[int] = None
    test_ratio: float = 0.2
    val_ratio: float = 0.1
    
    # æ¨¡å‹é…ç½®
    bins: int = 50
    condition_dim: int = 17
    cond_encoder_d_model: int = 128
    residual_hidden_size: int = 256
    residual_num_layers: int = 8
    diffusion_steps: int = 1000
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 8
    num_epochs: int = 50
    learning_rate: float = 2e-4
    weight_decay: float = 1e-4
    gradient_clip_norm: float = 1.0
    
    # é¢„è®­ç»ƒé…ç½®
    pretrain_condition_encoder: bool = True
    pretrain_epochs: int = 20
    pretrain_lr: float = 1e-3
    
    # éªŒè¯å’Œä¿å­˜
    validate_every: int = 5
    save_every: int = 10
    num_workers: int = 0
    
    # æŸå¤±æƒé‡
    residual_loss_weight: float = 1.0
    fusion_loss_weight: float = 0.1
    contrastive_loss_weight: float = 0.05
    
    # æ¨ç†é…ç½®
    inference_num_steps: int = 50
    inference_guidance_scale: float = 1.0
    inference_temperature: float = 1.0


class DualGeneratorTrainer:
    """åŒç”Ÿæˆå™¨è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = config
        
        # è®¾ç½®è®¾å¤‡
        if config.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(config.device)
        
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config.output_dir) / config.experiment_name
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        with open(self.output_dir / "config.json", 'w') as f:
            json.dump(config.__dict__, f, indent=2)
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.writer = SummaryWriter(self.output_dir / "tensorboard")
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader, self.test_loader = self._build_dataloaders()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._build_model()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = self._build_optimizer()
        
        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.global_step = 0
        self.best_val_loss = float('inf')
        
        # è¯„ä¼°å™¨
        self.evaluator = EvaluationMetrics()
        
        print(f"ğŸ¯ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"  å®éªŒåç§°: {config.experiment_name}")
        print(f"  è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(self.train_loader.dataset)}")
        print(f"  éªŒè¯æ ·æœ¬: {len(self.val_loader.dataset)}")
        print(f"  æµ‹è¯•æ ·æœ¬: {len(self.test_loader.dataset)}")
    
    def _build_dataloaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """æ„å»ºæ•°æ®åŠ è½½å™¨"""
        print("ğŸ“‚ æ„å»ºæ•°æ®åŠ è½½å™¨...")
        
        # è®­ç»ƒé›†
        train_dataset = DualPolyDataset(
            csv_path=self.config.csv_path,
            max_length=self.config.bins,
            max_samples=self.config.max_samples,
            split='train',
            test_ratio=self.config.test_ratio,
            val_ratio=self.config.val_ratio
        )
        
        # éªŒè¯é›†
        val_dataset = DualPolyDataset(
            csv_path=self.config.csv_path,
            max_length=self.config.bins,
            max_samples=self.config.max_samples,
            split='val',
            test_ratio=self.config.test_ratio,
            val_ratio=self.config.val_ratio
        )
        
        # æµ‹è¯•é›†
        test_dataset = DualPolyDataset(
            csv_path=self.config.csv_path,
            max_length=self.config.bins,
            max_samples=self.config.max_samples,
            split='test',
            test_ratio=self.config.test_ratio,
            val_ratio=self.config.val_ratio
        )
        
        # åˆ›å»ºDataLoader
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=self.config.num_workers,
            collate_fn=collate_dual_poly,
            pin_memory=True,
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            collate_fn=collate_dual_poly,
            pin_memory=True
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            collate_fn=collate_dual_poly,
            pin_memory=True
        )
        
        return train_loader, val_loader, test_loader
    
    def _build_model(self) -> DualGeneratorModel:
        """æ„å»ºæ¨¡å‹"""
        print("ğŸ—ï¸ æ„å»ºåŒç”Ÿæˆå™¨æ¨¡å‹...")
        
        model = DualGeneratorModel(
            bins=self.config.bins,
            condition_dim=self.config.condition_dim,
            cond_encoder_d_model=self.config.cond_encoder_d_model,
            residual_hidden_size=self.config.residual_hidden_size,
            residual_num_layers=self.config.residual_num_layers,
            diffusion_steps=self.config.diffusion_steps
        ).to(self.device)
        
        return model
    
    def _build_optimizer(self) -> optim.Optimizer:
        """æ„å»ºä¼˜åŒ–å™¨"""
        return optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
    
    def pretrain_condition_encoder(self):
        """é¢„è®­ç»ƒæ¡ä»¶ç¼–ç å™¨"""
        if not self.config.pretrain_condition_encoder:
            print("â­ï¸ è·³è¿‡æ¡ä»¶ç¼–ç å™¨é¢„è®­ç»ƒ")
            return
        
        print(f"ğŸ¯ å¼€å§‹æ¡ä»¶ç¼–ç å™¨é¢„è®­ç»ƒ ({self.config.pretrain_epochs} epochs)...")
        
        # åˆ›å»ºé¢„è®­ç»ƒä¼˜åŒ–å™¨
        pretrain_optimizer = optim.AdamW(
            self.model.condition_encoder.parameters(),
            lr=self.config.pretrain_lr,
            weight_decay=self.config.weight_decay
        )
        
        self.model.condition_encoder.train()
        
        for epoch in range(self.config.pretrain_epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            for batch in self.train_loader:
                condition_features = batch['condition_features'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                results = self.model.condition_encoder(condition_features)
                
                # å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆè‡ªç›‘ç£ï¼‰
                proj_emb1 = results['proj_emb']
                
                # åˆ›å»ºå¢å¼ºç‰ˆæœ¬ï¼ˆæ·»åŠ å™ªå£°ï¼‰
                noise = torch.randn_like(condition_features) * 0.1
                augmented_features = condition_features + noise
                augmented_results = self.model.condition_encoder(augmented_features)
                proj_emb2 = augmented_results['proj_emb']
                
                # è®¡ç®—å¯¹æ¯”æŸå¤±
                contrastive_loss = self.model.condition_encoder.compute_contrastive_loss(
                    proj_emb1, proj_emb2
                )
                
                # åå‘ä¼ æ’­
                pretrain_optimizer.zero_grad()
                contrastive_loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    self.model.condition_encoder.parameters(), 
                    self.config.gradient_clip_norm
                )
                pretrain_optimizer.step()
                
                epoch_loss += contrastive_loss.item()
                num_batches += 1
            
            avg_loss = epoch_loss / num_batches
            print(f"  é¢„è®­ç»ƒ Epoch {epoch+1:2d}: å¯¹æ¯”æŸå¤± = {avg_loss:.6f}")
            
            # è®°å½•æ—¥å¿—
            self.writer.add_scalar('Pretrain/ContrastiveLoss', avg_loss, epoch)
        
        print("âœ… æ¡ä»¶ç¼–ç å™¨é¢„è®­ç»ƒå®Œæˆ!")
        
        # ä¿å­˜é¢„è®­ç»ƒæ¨¡å‹
        pretrain_path = self.output_dir / "condition_encoder_pretrained.pt"
        torch.save({
            'model_state_dict': self.model.condition_encoder.state_dict(),
            'optimizer_state_dict': pretrain_optimizer.state_dict(),
            'epoch': self.config.pretrain_epochs,
            'loss': avg_loss
        }, pretrain_path)
        
        print(f"ğŸ’¾ é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜: {pretrain_path}")
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        epoch_losses = {
            'total_loss': 0.0,
            'residual_loss': 0.0,
            'fusion_loss': 0.0,
            'contrastive_loss': 0.0
        }
        num_batches = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            condition_features = batch['condition_features'].to(self.device)
            residual_targets = batch['residual_targets'].to(self.device)
            block_distributions = batch['block_distributions'].to(self.device)
            sequences_batch = batch['sequences']
            mayo_params_batch = batch['mayo_lewis_params']
            
            # å‰å‘ä¼ æ’­ - è®­ç»ƒæ¨¡å¼
            train_results = self.model(
                condition_features=condition_features,
                sequences_batch=sequences_batch,
                mode='training',
                residual_targets=residual_targets
            )
            
            # è®¡ç®—æ®‹å·®æŸå¤±
            residual_loss = train_results['residual_loss']
            
            # æ¨ç†æ¨¡å¼è·å–èåˆç»“æœ
            with torch.no_grad():
                inference_results = self.model(
                    condition_features=condition_features,
                    sequences_batch=sequences_batch,
                    mode='inference',
                    num_steps=10  # å‡å°‘æ­¥æ•°ä»¥åŠ é€Ÿè®­ç»ƒ
                )
            
            # è®¡ç®—èåˆæŸå¤±
            fusion_loss_dict = self.model.fusion_module.compute_fusion_loss(
                fused_dist=inference_results['fused_distribution'],
                target_dist=block_distributions,
                confidence=inference_results['confidence'],
                quality_score=inference_results['quality_score']
            )
            fusion_loss = fusion_loss_dict['total_loss']
            
            # è®¡ç®—å¯¹æ¯”æŸå¤±
            encoding_results = train_results['encoding_results']
            proj_emb = encoding_results['projection_embedding']
            
            # åˆ›å»ºå¯¹æ¯”ç›®æ ‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            contrastive_loss = torch.tensor(0.0, device=self.device)
            if proj_emb.size(0) > 1:
                # ä½¿ç”¨æ‰¹æ¬¡å†…çš„å¯¹æ¯”å­¦ä¹ 
                contrastive_loss = self.model.condition_encoder.compute_contrastive_loss(
                    proj_emb, proj_emb.roll(1, dims=0)
                )
            
            # æ€»æŸå¤±
            total_loss = (
                self.config.residual_loss_weight * residual_loss +
                self.config.fusion_loss_weight * fusion_loss +
                self.config.contrastive_loss_weight * contrastive_loss
            )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config.gradient_clip_norm
            )
            self.optimizer.step()
            
            # ç´¯ç§¯æŸå¤±
            epoch_losses['total_loss'] += total_loss.item()
            epoch_losses['residual_loss'] += residual_loss.item()
            epoch_losses['fusion_loss'] += fusion_loss.item()
            epoch_losses['contrastive_loss'] += contrastive_loss.item()
            num_batches += 1
            
            self.global_step += 1
            
            # è®°å½•è®­ç»ƒæ—¥å¿—
            if batch_idx % 50 == 0:
                print(f"  Batch {batch_idx:3d}: "
                      f"Total={total_loss.item():.4f}, "
                      f"Residual={residual_loss.item():.4f}, "
                      f"Fusion={fusion_loss.item():.4f}")
                
                # è®°å½•åˆ°tensorboard
                self.writer.add_scalar('Train/BatchLoss', total_loss.item(), self.global_step)
        
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in epoch_losses:
            epoch_losses[key] /= num_batches
        
        return epoch_losses
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        
        val_losses = {
            'total_loss': 0.0,
            'kl_divergence': 0.0,
            'mse': 0.0
        }
        num_batches = 0
        
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch in self.val_loader:
                condition_features = batch['condition_features'].to(self.device)
                block_distributions = batch['block_distributions'].to(self.device)
                sequences_batch = batch['sequences']
                
                # æ¨ç†
                results = self.model(
                    condition_features=condition_features,
                    sequences_batch=sequences_batch,
                    mode='inference',
                    num_steps=self.config.inference_num_steps
                )
                
                fused_dist = results['fused_distribution']
                
                # è®¡ç®—æŸå¤±
                kl_loss = torch.nn.functional.kl_div(
                    torch.log(fused_dist + 1e-8),
                    block_distributions,
                    reduction='batchmean'
                )
                mse_loss = torch.nn.functional.mse_loss(fused_dist, block_distributions)
                
                val_losses['kl_divergence'] += kl_loss.item()
                val_losses['mse'] += mse_loss.item()
                val_losses['total_loss'] += kl_loss.item() + mse_loss.item()
                
                # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡
                all_predictions.append(fused_dist.cpu())
                all_targets.append(block_distributions.cpu())
                
                num_batches += 1
        
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in val_losses:
            val_losses[key] /= num_batches
        
        # è®¡ç®—è¯¦ç»†è¯„ä¼°æŒ‡æ ‡
        all_predictions = torch.cat(all_predictions, dim=0)
        all_targets = torch.cat(all_targets, dim=0)
        
        detailed_metrics = self.evaluator.compute_metrics(
            all_predictions.numpy(),
            all_targets.numpy()
        )
        
        val_losses.update(detailed_metrics)
        
        return val_losses
    
    def save_checkpoint(self, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config.__dict__
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = self.output_dir / f"checkpoint_epoch_{self.epoch:03d}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = self.output_dir / "best_model.pt"
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"ğŸš€ å¼€å§‹åŒç”Ÿæˆå™¨è®­ç»ƒ...")
        print(f"  æ€»epochs: {self.config.num_epochs}")
        print(f"  æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        print(f"  å­¦ä¹ ç‡: {self.config.learning_rate}")
        
        # é¢„è®­ç»ƒæ¡ä»¶ç¼–ç å™¨
        self.pretrain_condition_encoder()
        
        # ä¸»è®­ç»ƒå¾ªç¯
        for epoch in range(1, self.config.num_epochs + 1):
            self.epoch = epoch
            start_time = time.time()
            
            print(f"\nğŸ“ˆ Epoch {epoch}/{self.config.num_epochs}")
            
            # è®­ç»ƒ
            train_losses = self.train_epoch()
            
            # è®°å½•è®­ç»ƒæŸå¤±
            for loss_name, loss_value in train_losses.items():
                self.writer.add_scalar(f'Train/{loss_name}', loss_value, epoch)
            
            epoch_time = time.time() - start_time
            
            print(f"  è®­ç»ƒæŸå¤±: Total={train_losses['total_loss']:.6f}, "
                  f"Residual={train_losses['residual_loss']:.6f}, "
                  f"æ—¶é—´={epoch_time:.1f}s")
            
            # éªŒè¯
            if epoch % self.config.validate_every == 0:
                print(f"  ğŸ” éªŒè¯ä¸­...")
                val_losses = self.validate()
                
                # è®°å½•éªŒè¯æŸå¤±
                for loss_name, loss_value in val_losses.items():
                    self.writer.add_scalar(f'Val/{loss_name}', loss_value, epoch)
                
                print(f"  éªŒè¯æŸå¤±: KL={val_losses['kl_divergence']:.6f}, "
                      f"MSE={val_losses['mse']:.6f}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                current_val_loss = val_losses['total_loss']
                is_best = current_val_loss < self.best_val_loss
                if is_best:
                    self.best_val_loss = current_val_loss
                    print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if epoch % self.config.save_every == 0 or is_best:
                    self.save_checkpoint(is_best=is_best)
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
        print(f"  è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # å…³é—­tensorboard
        self.writer.close()


def test_trainer():
    """æµ‹è¯•è®­ç»ƒå™¨"""
    print("ğŸ§ª æµ‹è¯•DualGeneratorTrainer...")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = TrainingConfig(
        experiment_name="test_dual_generator",
        csv_path="PolyGen-F06C/data/copolymer.csv",
        max_samples=50,  # é™åˆ¶æ ·æœ¬æ•°
        bins=20,
        batch_size=2,
        num_epochs=2,
        pretrain_epochs=1,
        validate_every=1,
        save_every=1,
        diffusion_steps=10,
        cond_encoder_d_model=32,
        residual_hidden_size=64,
        residual_num_layers=2
    )
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = DualGeneratorTrainer(config)
        
        # è¿è¡Œè®­ç»ƒ
        trainer.train()
        
        print("âœ… DualGeneratorTraineræµ‹è¯•é€šè¿‡!")
        
    except Exception as e:
        print(f"âŒ DualGeneratorTraineræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_trainer()
