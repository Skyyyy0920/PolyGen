#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒå’Œè¯„ä¼°å·¥å…·å‡½æ•°
"""

import numpy as np
import torch
from typing import Dict, Tuple, List
from scipy.stats import wasserstein_distance
from sklearn.metrics import mean_squared_error


class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self):
        pass
    
    def kl_divergence(self, p: np.ndarray, q: np.ndarray, eps: float = 1e-8) -> float:
        """
        è®¡ç®—KLæ•£åº¦
        
        Args:
            p: çœŸå®åˆ†å¸ƒ [batch_size, bins] æˆ– [bins]
            q: é¢„æµ‹åˆ†å¸ƒ [batch_size, bins] æˆ– [bins]
            eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°
            
        Returns:
            KLæ•£åº¦å€¼
        """
        # ç¡®ä¿éè´Ÿå¹¶å½’ä¸€åŒ–
        p = np.maximum(p, eps)
        q = np.maximum(q, eps)
        
        if p.ndim == 1:
            p = p / np.sum(p)
            q = q / np.sum(q)
            return np.sum(p * np.log(p / q))
        else:
            # æ‰¹é‡è®¡ç®—
            p = p / np.sum(p, axis=1, keepdims=True)
            q = q / np.sum(q, axis=1, keepdims=True)
            return np.mean(np.sum(p * np.log(p / q), axis=1))
    
    def js_divergence(self, p: np.ndarray, q: np.ndarray) -> float:
        """
        è®¡ç®—JSæ•£åº¦ï¼ˆå¯¹ç§°ç‰ˆæœ¬çš„KLæ•£åº¦ï¼‰
        
        Args:
            p: çœŸå®åˆ†å¸ƒ
            q: é¢„æµ‹åˆ†å¸ƒ
            
        Returns:
            JSæ•£åº¦å€¼
        """
        # è®¡ç®—ä¸­ç‚¹åˆ†å¸ƒ
        m = 0.5 * (p + q)
        
        # JSæ•£åº¦ = 0.5 * KL(p||m) + 0.5 * KL(q||m)
        return 0.5 * self.kl_divergence(p, m) + 0.5 * self.kl_divergence(q, m)
    
    def earth_mover_distance(self, p: np.ndarray, q: np.ndarray) -> float:
        """
        è®¡ç®—Earth Mover's Distance (Wassersteinè·ç¦»)
        
        Args:
            p: çœŸå®åˆ†å¸ƒ
            q: é¢„æµ‹åˆ†å¸ƒ
            
        Returns:
            EMDå€¼
        """
        if p.ndim == 1:
            # å•ä¸ªåˆ†å¸ƒ
            x = np.arange(len(p))
            return wasserstein_distance(x, x, p, q)
        else:
            # æ‰¹é‡è®¡ç®—
            emds = []
            x = np.arange(p.shape[1])
            for i in range(p.shape[0]):
                emd = wasserstein_distance(x, x, p[i], q[i])
                emds.append(emd)
            return np.mean(emds)
    
    def mean_squared_error(self, p: np.ndarray, q: np.ndarray) -> float:
        """è®¡ç®—å‡æ–¹è¯¯å·®"""
        return mean_squared_error(p.flatten(), q.flatten())
    
    def mean_absolute_error(self, p: np.ndarray, q: np.ndarray) -> float:
        """è®¡ç®—å¹³å‡ç»å¯¹è¯¯å·®"""
        return np.mean(np.abs(p - q))
    
    def peak_position_accuracy(self, p: np.ndarray, q: np.ndarray) -> float:
        """
        è®¡ç®—å³°å€¼ä½ç½®å‡†ç¡®ç‡
        
        Args:
            p: çœŸå®åˆ†å¸ƒ
            q: é¢„æµ‹åˆ†å¸ƒ
            
        Returns:
            å³°å€¼ä½ç½®åŒ¹é…çš„æ¯”ä¾‹
        """
        if p.ndim == 1:
            return float(np.argmax(p) == np.argmax(q))
        else:
            peak_p = np.argmax(p, axis=1)
            peak_q = np.argmax(q, axis=1)
            return np.mean(peak_p == peak_q)
    
    def distribution_moments(self, dist: np.ndarray) -> Dict[str, float]:
        """
        è®¡ç®—åˆ†å¸ƒçš„ç»Ÿè®¡çŸ©
        
        Args:
            dist: åˆ†å¸ƒ [batch_size, bins] æˆ– [bins]
            
        Returns:
            ç»Ÿè®¡çŸ©å­—å…¸
        """
        if dist.ndim == 1:
            x = np.arange(1, len(dist) + 1)  # å—é•¿åº¦ä»1å¼€å§‹
            
            # å½’ä¸€åŒ–
            dist_norm = dist / np.sum(dist)
            
            # è®¡ç®—å„é˜¶çŸ©
            mean = np.sum(x * dist_norm)
            variance = np.sum((x - mean) ** 2 * dist_norm)
            std = np.sqrt(variance)
            
            # ååº¦å’Œå³°åº¦
            if std > 0:
                skewness = np.sum(((x - mean) / std) ** 3 * dist_norm)
                kurtosis = np.sum(((x - mean) / std) ** 4 * dist_norm) - 3
            else:
                skewness = 0.0
                kurtosis = 0.0
            
            return {
                'mean': mean,
                'std': std,
                'variance': variance,
                'skewness': skewness,
                'kurtosis': kurtosis
            }
        else:
            # æ‰¹é‡è®¡ç®—å¹³å‡å€¼
            moments_list = [self.distribution_moments(dist[i]) for i in range(dist.shape[0])]
            
            avg_moments = {}
            for key in moments_list[0].keys():
                avg_moments[key] = np.mean([m[key] for m in moments_list])
            
            return avg_moments
    
    def compute_metrics(self, predictions: np.ndarray, targets: np.ndarray) -> Dict[str, float]:
        """
        è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹åˆ†å¸ƒ [batch_size, bins]
            targets: çœŸå®åˆ†å¸ƒ [batch_size, bins]
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        # åŸºæœ¬è¯¯å·®æŒ‡æ ‡
        metrics['kl_divergence'] = self.kl_divergence(targets, predictions)
        metrics['js_divergence'] = self.js_divergence(targets, predictions)
        metrics['earth_mover_distance'] = self.earth_mover_distance(targets, predictions)
        metrics['mse'] = self.mean_squared_error(targets, predictions)
        metrics['mae'] = self.mean_absolute_error(targets, predictions)
        
        # å³°å€¼ä½ç½®å‡†ç¡®ç‡
        metrics['peak_accuracy'] = self.peak_position_accuracy(targets, predictions)
        
        # åˆ†å¸ƒçŸ©å¯¹æ¯”
        target_moments = self.distribution_moments(targets)
        pred_moments = self.distribution_moments(predictions)
        
        for key in target_moments.keys():
            metrics[f'target_{key}'] = target_moments[key]
            metrics[f'pred_{key}'] = pred_moments[key]
            metrics[f'{key}_error'] = abs(target_moments[key] - pred_moments[key])
        
        return metrics
    
    def compute_mayo_lewis_comparison(self, 
                                    predictions: np.ndarray,
                                    targets: np.ndarray,
                                    theoretical: np.ndarray) -> Dict[str, float]:
        """
        è®¡ç®—ä¸Mayo-Lewisç†è®ºçš„å¯¹æ¯”æŒ‡æ ‡
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ [batch_size, bins]
            targets: çœŸå®åˆ†å¸ƒ [batch_size, bins]
            theoretical: Mayo-Lewisç†è®ºåˆ†å¸ƒ [batch_size, bins]
            
        Returns:
            å¯¹æ¯”æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        # æ¨¡å‹ vs çœŸå®
        metrics['model_vs_target_kl'] = self.kl_divergence(targets, predictions)
        metrics['model_vs_target_emd'] = self.earth_mover_distance(targets, predictions)
        
        # ç†è®º vs çœŸå®
        metrics['theory_vs_target_kl'] = self.kl_divergence(targets, theoretical)
        metrics['theory_vs_target_emd'] = self.earth_mover_distance(targets, theoretical)
        
        # æ¨¡å‹ vs ç†è®º
        metrics['model_vs_theory_kl'] = self.kl_divergence(theoretical, predictions)
        metrics['model_vs_theory_emd'] = self.earth_mover_distance(theoretical, predictions)
        
        # æ”¹è¿›åº¦è®¡ç®—
        kl_improvement = (metrics['theory_vs_target_kl'] - metrics['model_vs_target_kl']) / metrics['theory_vs_target_kl'] * 100
        emd_improvement = (metrics['theory_vs_target_emd'] - metrics['model_vs_target_emd']) / metrics['theory_vs_target_emd'] * 100
        
        metrics['kl_improvement_percent'] = kl_improvement
        metrics['emd_improvement_percent'] = emd_improvement
        
        return metrics


def create_learning_rate_scheduler(optimizer: torch.optim.Optimizer, 
                                 scheduler_type: str = 'cosine',
                                 **kwargs) -> torch.optim.lr_scheduler._LRScheduler:
    """
    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    
    Args:
        optimizer: ä¼˜åŒ–å™¨
        scheduler_type: è°ƒåº¦å™¨ç±»å‹
        **kwargs: è°ƒåº¦å™¨å‚æ•°
        
    Returns:
        å­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    if scheduler_type == 'cosine':
        return torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=kwargs.get('T_max', 100),
            eta_min=kwargs.get('eta_min', 1e-6)
        )
    elif scheduler_type == 'step':
        return torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=kwargs.get('step_size', 30),
            gamma=kwargs.get('gamma', 0.1)
        )
    elif scheduler_type == 'exponential':
        return torch.optim.lr_scheduler.ExponentialLR(
            optimizer,
            gamma=kwargs.get('gamma', 0.95)
        )
    elif scheduler_type == 'reduce_on_plateau':
        return torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=kwargs.get('factor', 0.5),
            patience=kwargs.get('patience', 10),
            verbose=True
        )
    else:
        raise ValueError(f"Unknown scheduler type: {scheduler_type}")


def compute_gradient_norm(model: torch.nn.Module) -> float:
    """è®¡ç®—æ¨¡å‹æ¢¯åº¦èŒƒæ•°"""
    total_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    return total_norm ** 0.5


def log_model_weights(model: torch.nn.Module, writer, step: int):
    """è®°å½•æ¨¡å‹æƒé‡åˆ†å¸ƒåˆ°tensorboard"""
    for name, param in model.named_parameters():
        if param.requires_grad and param.grad is not None:
            writer.add_histogram(f'weights/{name}', param.data, step)
            writer.add_histogram(f'gradients/{name}', param.grad.data, step)


def save_predictions_visualization(predictions: np.ndarray,
                                 targets: np.ndarray,
                                 theoretical: np.ndarray,
                                 save_path: str,
                                 num_samples: int = 9):
    """
    ä¿å­˜é¢„æµ‹ç»“æœå¯è§†åŒ–
    
    Args:
        predictions: é¢„æµ‹åˆ†å¸ƒ
        targets: çœŸå®åˆ†å¸ƒ
        theoretical: ç†è®ºåˆ†å¸ƒ
        save_path: ä¿å­˜è·¯å¾„
        num_samples: å¯è§†åŒ–æ ·æœ¬æ•°
    """
    import matplotlib.pyplot as plt
    
    num_samples = min(num_samples, predictions.shape[0])
    
    fig, axes = plt.subplots(3, 3, figsize=(15, 12))
    axes = axes.flatten()
    
    for i in range(num_samples):
        ax = axes[i]
        
        x = np.arange(1, predictions.shape[1] + 1)
        
        ax.plot(x, targets[i], 'k-', linewidth=2, label='Ground Truth', alpha=0.8)
        ax.plot(x, predictions[i], 'b-', linewidth=2, label='Prediction', alpha=0.7)
        ax.plot(x, theoretical[i], 'r--', linewidth=2, label='Mayo-Lewis', alpha=0.7)
        
        ax.set_title(f'Sample {i+1}')
        ax.set_xlabel('Block Length')
        ax.set_ylabel('Probability')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


def test_evaluation_metrics():
    """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡"""
    print("ğŸ§ª æµ‹è¯•EvaluationMetrics...")
    
    evaluator = EvaluationMetrics()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    batch_size = 5
    bins = 20
    
    # çœŸå®åˆ†å¸ƒï¼ˆå‡ ä½•åˆ†å¸ƒï¼‰
    x = np.arange(1, bins + 1)
    targets = []
    for i in range(batch_size):
        p = 0.2 + 0.1 * i  # ä¸åŒçš„å‚æ•°
        dist = (1 - p) * (p ** (x - 1))
        dist = dist / np.sum(dist)
        targets.append(dist)
    targets = np.stack(targets)
    
    # é¢„æµ‹åˆ†å¸ƒï¼ˆæ·»åŠ å™ªå£°ï¼‰
    predictions = targets + np.random.normal(0, 0.02, targets.shape)
    predictions = np.maximum(predictions, 0)
    predictions = predictions / np.sum(predictions, axis=1, keepdims=True)
    
    # ç†è®ºåˆ†å¸ƒï¼ˆç¨æœ‰åå·®ï¼‰
    theoretical = targets * 0.9 + 0.1 * np.ones_like(targets) / bins
    theoretical = theoretical / np.sum(theoretical, axis=1, keepdims=True)
    
    # è®¡ç®—æŒ‡æ ‡
    print("åŸºæœ¬è¯„ä¼°æŒ‡æ ‡:")
    basic_metrics = evaluator.compute_metrics(predictions, targets)
    for key, value in basic_metrics.items():
        print(f"  {key}: {value:.6f}")
    
    print("\nMayo-Lewiså¯¹æ¯”æŒ‡æ ‡:")
    comparison_metrics = evaluator.compute_mayo_lewis_comparison(predictions, targets, theoretical)
    for key, value in comparison_metrics.items():
        print(f"  {key}: {value:.6f}")
    
    print("âœ… EvaluationMetricsæµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    test_evaluation_metrics()
