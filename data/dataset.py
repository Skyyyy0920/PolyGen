#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒç”Ÿæˆå™¨èšåˆç‰©æ•°æ®é›†

æ”¯æŒåŸå§‹PolyGen-F06Cæ•°æ®æ ¼å¼ï¼Œå¹¶æ‰©å±•æ”¯æŒåŒç”Ÿæˆå™¨è®­ç»ƒ
"""

import ast
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset
from typing import List, Dict, Tuple, Optional, Union
from pathlib import Path

from .mayo_lewis import MayoLewisCalculator


class DualPolyDataset(Dataset):
    """
    åŒç”Ÿæˆå™¨èšåˆç‰©æ•°æ®é›†
    
    æ”¯æŒï¼š
    1. åŠ è½½PolyGen-F06Cæ ¼å¼çš„copolymer.csvæ•°æ®
    2. æå–Mayo-Lewisç†è®ºåˆ†å¸ƒ
    3. è®¡ç®—æ®‹å·®ç›®æ ‡ï¼ˆå®é™…åˆ†å¸ƒ - ç†è®ºåˆ†å¸ƒï¼‰
    4. æ¡ä»¶ç‰¹å¾æå–å’Œç¼–ç 
    """
    
    def __init__(self, 
                 csv_path: str,
                 max_length: int = 50,
                 max_samples: Optional[int] = None,
                 split: str = 'train',
                 test_ratio: float = 0.2,
                 val_ratio: float = 0.1,
                 random_seed: int = 42):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            csv_path: copolymer.csvæ–‡ä»¶è·¯å¾„
            max_length: æœ€å¤§å—é•¿åº¦
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            split: æ•°æ®åˆ†å‰² ('train', 'val', 'test')
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            random_seed: éšæœºç§å­
        """
        self.csv_path = Path(csv_path)
        self.max_length = max_length
        self.split = split
        
        # åˆå§‹åŒ–Mayo-Lewisè®¡ç®—å™¨
        self.mayo_lewis_calc = MayoLewisCalculator(max_length=max_length)
        
        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        self.data = self._load_data(max_samples)
        
        # æ•°æ®åˆ†å‰²
        self.data = self._split_data(test_ratio, val_ratio, random_seed)
        
        print(f"ğŸ“Š DualPolyDatasetåˆå§‹åŒ–å®Œæˆ:")
        print(f"  æ•°æ®æ–‡ä»¶: {self.csv_path}")
        print(f"  åˆ†å‰²: {self.split}")
        print(f"  æ ·æœ¬æ•°: {len(self.data)}")
        print(f"  æœ€å¤§å—é•¿åº¦: {self.max_length}")
    
    def _load_data(self, max_samples: Optional[int]) -> List[Dict]:
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {self.csv_path}")
        
        if not self.csv_path.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.csv_path}")
        
        # è¯»å–CSV
        try:
            df = pd.read_csv(self.csv_path, encoding='utf-8')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(self.csv_path, encoding='gbk')
            except UnicodeDecodeError:
                df = pd.read_csv(self.csv_path, encoding='latin-1')
        
        # åˆ é™¤å®Œå…¨ä¸ºç©ºçš„è¡Œ
        df = df.dropna(how='all')
        
        if max_samples:
            df = df.head(max_samples)
            print(f"  é™åˆ¶æ ·æœ¬æ•°: {max_samples}")
        
        print(f"  åŸå§‹æ ·æœ¬æ•°: {len(df)}")
        print(f"  æ•°æ®åˆ—: {list(df.columns)}")
        
        if len(df) == 0:
            raise ValueError("æ•°æ®æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¡Œ")
        
        # é¢„å¤„ç†æ•°æ®
        processed_data = []
        
        for idx, row in df.iterrows():
            try:
                # è·³è¿‡ç©ºè¡Œæˆ–æ— æ•ˆè¡Œ
                if pd.isna(row.get('seq')) or pd.isna(row.get('block_dist')):
                    continue
                
                # è§£æåºåˆ—æ•°æ®
                sequences = self._parse_sequences(row['seq'])
                if not sequences:
                    continue
                
                # è§£æå—é•¿åº¦åˆ†å¸ƒ
                block_dist = self._parse_block_distribution(row['block_dist'])
                if block_dist is None or len(block_dist) == 0:
                    continue
                
                # ç¡®ä¿åˆ†å¸ƒé•¿åº¦åŒ¹é…
                if len(block_dist) > self.max_length:
                    block_dist = block_dist[:self.max_length]
                elif len(block_dist) < self.max_length:
                    # ç”¨é›¶å¡«å……
                    padding = np.zeros(self.max_length - len(block_dist))
                    block_dist = np.concatenate([block_dist, padding])
                
                # å½’ä¸€åŒ–åˆ†å¸ƒ
                if np.sum(block_dist) > 0:
                    block_dist = block_dist / np.sum(block_dist)
                
                # æå–æ¡ä»¶ç‰¹å¾
                condition_features = self._extract_condition_features(row, sequences)
                
                # è®¡ç®—Mayo-Lewisç†è®ºåˆ†å¸ƒ
                theoretical_dist = self.mayo_lewis_calc.calculate_theoretical_distribution(sequences)
                
                # è®¡ç®—æ®‹å·®ç›®æ ‡
                residual_target = block_dist - theoretical_dist
                
                # æ„é€ æ ·æœ¬
                sample = {
                    'idx': idx,
                    'sequences': sequences,
                    'block_distribution': block_dist.astype(np.float32),
                    'theoretical_distribution': theoretical_dist.astype(np.float32),
                    'residual_target': residual_target.astype(np.float32),
                    'condition_features': condition_features.astype(np.float32),
                    'mayo_lewis_params': self.mayo_lewis_calc.extract_sequence_statistics(sequences),
                    'metadata': {
                        'name': row.get('name', f'sample_{idx}'),
                        'activation': row.get('activation', 0),
                        'temp': row.get('Temp', 0),
                        'prob_AA': row.get('probAA', 0),
                        'prob_BB': row.get('probBB', 0),
                    }
                }
                
                processed_data.append(sample)
                
            except Exception as e:
                print(f"  è­¦å‘Š: æ ·æœ¬{idx}å¤„ç†å¤±è´¥: {e}")
                continue
        
        print(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {len(processed_data)}")
        return processed_data
    
    def _parse_sequences(self, seq_str: str) -> List[str]:
        """è§£æåºåˆ—å­—ç¬¦ä¸²"""
        try:
            if isinstance(seq_str, str):
                # å°è¯•è§£æä¸ºPythonåˆ—è¡¨
                sequences = ast.literal_eval(seq_str)
                if isinstance(sequences, list):
                    return [str(seq) for seq in sequences if seq and isinstance(seq, str)]
            return []
        except:
            return []
    
    def _parse_block_distribution(self, block_dist_str: str) -> Optional[np.ndarray]:
        """è§£æå—é•¿åº¦åˆ†å¸ƒ"""
        try:
            if isinstance(block_dist_str, str):
                # è§£æä¸ºnumpyæ•°ç»„
                parsed = ast.literal_eval(block_dist_str)
                if isinstance(parsed, list) and len(parsed) == 2:
                    # æ ¼å¼: [lengths_array, probs_array]
                    lengths, probs = parsed
                    if isinstance(probs, (list, np.ndarray)):
                        return np.array(probs, dtype=np.float32)
            return None
        except:
            return None
    
    def _extract_condition_features(self, row: pd.Series, sequences: List[str]) -> np.ndarray:
        """æå–æ¡ä»¶ç‰¹å¾"""
        # åŸºç¡€å®éªŒæ¡ä»¶
        features = [
            float(row.get('activation', 0)),  # æ´»åŒ–èƒ½
            float(row.get('Temp', 0)),        # æ¸©åº¦
            float(row.get('probAA', 0)),      # AAè½¬æ¢æ¦‚ç‡
            float(row.get('probBB', 0)),      # BBè½¬æ¢æ¦‚ç‡
            float(row.get('probAABB', 0)),    # AABBè½¬æ¢æ¦‚ç‡
            float(row.get('probAB', 0)),      # ABè½¬æ¢æ¦‚ç‡
        ]
        
        # åºåˆ—ç»Ÿè®¡ç‰¹å¾
        if sequences:
            all_seq = ''.join(sequences)
            seq_length = len(all_seq)
            f_A = all_seq.count('A') / seq_length if seq_length > 0 else 0.5
            
            # è½¬æ¢é¢‘ç‡ï¼ˆå¤æ‚åº¦æŒ‡æ ‡ï¼‰
            transitions = 0
            for seq in sequences:
                for i in range(len(seq) - 1):
                    if seq[i] != seq[i + 1]:
                        transitions += 1
            
            transition_freq = transitions / max(1, seq_length - len(sequences))
            
            features.extend([
                f_A,               # å•ä½“Aæ‘©å°”åˆ†æ•°
                1 - f_A,          # å•ä½“Bæ‘©å°”åˆ†æ•°  
                seq_length,       # æ€»åºåˆ—é•¿åº¦
                len(sequences),   # åºåˆ—æ•°é‡
                transition_freq,  # è½¬æ¢é¢‘ç‡
            ])
        else:
            features.extend([0.5, 0.5, 0, 0, 0])
        
        # ç‰©ç†åŒ–å­¦å‚æ•°
        features.extend([
            float(row.get('epsAA', 0)),      # AAç›¸äº’ä½œç”¨èƒ½
            float(row.get('epsAB', 0)),      # ABç›¸äº’ä½œç”¨èƒ½
            float(row.get('epsBB', 0)),      # BBç›¸äº’ä½œç”¨èƒ½
            float(row.get('damp', 0)),       # é˜»å°¼ç³»æ•°
            float(row.get('angleA', 0)),     # Aè§’åº¦
            float(row.get('angleB', 0)),     # Bè§’åº¦
        ])
        
        return np.array(features, dtype=np.float32)
    
    def _split_data(self, test_ratio: float, val_ratio: float, random_seed: int) -> List[Dict]:
        """æ•°æ®åˆ†å‰²"""
        np.random.seed(random_seed)
        n_total = len(self.data)
        indices = np.random.permutation(n_total)
        
        n_test = int(test_ratio * n_total)
        n_val = int(val_ratio * n_total)
        n_train = n_total - n_test - n_val
        
        if self.split == 'train':
            selected_indices = indices[:n_train]
        elif self.split == 'val':
            selected_indices = indices[n_train:n_train + n_val]
        elif self.split == 'test':
            selected_indices = indices[n_train + n_val:]
        else:
            raise ValueError(f"æœªçŸ¥çš„split: {self.split}")
        
        return [self.data[i] for i in selected_indices]
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict:
        """è·å–å•ä¸ªæ ·æœ¬"""
        sample = self.data[idx]
        
        return {
            'idx': sample['idx'],
            'sequences': sample['sequences'],
            'block_distribution': torch.tensor(sample['block_distribution']),
            'theoretical_distribution': torch.tensor(sample['theoretical_distribution']),
            'residual_target': torch.tensor(sample['residual_target']),
            'condition_features': torch.tensor(sample['condition_features']),
            'mayo_lewis_params': sample['mayo_lewis_params'],
            'metadata': sample['metadata']
        }
    
    def get_feature_dim(self) -> int:
        """è·å–æ¡ä»¶ç‰¹å¾ç»´åº¦"""
        if len(self.data) > 0:
            return len(self.data[0]['condition_features'])
        return 17  # é»˜è®¤ç‰¹å¾ç»´åº¦
    
    def get_statistics(self) -> Dict:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.data:
            return {}
        
        block_dists = np.stack([sample['block_distribution'] for sample in self.data])
        theoretical_dists = np.stack([sample['theoretical_distribution'] for sample in self.data])
        residuals = np.stack([sample['residual_target'] for sample in self.data])
        
        return {
            'n_samples': len(self.data),
            'max_length': self.max_length,
            'feature_dim': self.get_feature_dim(),
            'block_dist_stats': {
                'mean': float(np.mean(block_dists)),
                'std': float(np.std(block_dists)),
                'min': float(np.min(block_dists)),
                'max': float(np.max(block_dists))
            },
            'theoretical_dist_stats': {
                'mean': float(np.mean(theoretical_dists)),
                'std': float(np.std(theoretical_dists)),
                'min': float(np.min(theoretical_dists)),
                'max': float(np.max(theoretical_dists))
            },
            'residual_stats': {
                'mean': float(np.mean(residuals)),
                'std': float(np.std(residuals)),
                'min': float(np.min(residuals)),
                'max': float(np.max(residuals))
            }
        }


def collate_dual_poly(batch: List[Dict]) -> Dict:
    """
    åŒç”Ÿæˆå™¨æ•°æ®é›†çš„collateå‡½æ•°
    
    Args:
        batch: æ‰¹é‡æ ·æœ¬åˆ—è¡¨
        
    Returns:
        æ‰¹é‡å¤„ç†åçš„æ•°æ®å­—å…¸
    """
    # æå–å„ä¸ªå­—æ®µ
    indices = [sample['idx'] for sample in batch]
    sequences_batch = [sample['sequences'] for sample in batch]
    
    # å †å å¼ é‡æ•°æ®
    block_distributions = torch.stack([sample['block_distribution'] for sample in batch])
    theoretical_distributions = torch.stack([sample['theoretical_distribution'] for sample in batch])
    residual_targets = torch.stack([sample['residual_target'] for sample in batch])
    condition_features = torch.stack([sample['condition_features'] for sample in batch])
    
    # æ”¶é›†å…ƒæ•°æ®
    mayo_lewis_params = [sample['mayo_lewis_params'] for sample in batch]
    metadata = [sample['metadata'] for sample in batch]
    
    return {
        'indices': indices,
        'sequences': sequences_batch,
        'block_distributions': block_distributions,
        'theoretical_distributions': theoretical_distributions,
        'residual_targets': residual_targets,
        'condition_features': condition_features,
        'mayo_lewis_params': mayo_lewis_params,
        'metadata': metadata
    }


def test_dataset():
    """æµ‹è¯•æ•°æ®é›†åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•DualPolyDataset...")
    
    # ä½¿ç”¨çœŸå®æ•°æ®è·¯å¾„
    csv_path = "PolyGen-F06C/data/copolymer.csv"
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        dataset = DualPolyDataset(
            csv_path=csv_path,
            max_length=30,
            max_samples=100,  # é™åˆ¶æ ·æœ¬æ•°ç”¨äºæµ‹è¯•
            split='train'
        )
        
        print(f"æ•°æ®é›†ç»Ÿè®¡:")
        stats = dataset.get_statistics()
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        from torch.utils.data import DataLoader
        
        dataloader = DataLoader(
            dataset, 
            batch_size=4, 
            shuffle=True, 
            collate_fn=collate_dual_poly
        )
        
        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        batch = next(iter(dataloader))
        
        print(f"\næ‰¹æ¬¡æµ‹è¯•:")
        print(f"  æ‰¹æ¬¡å¤§å°: {len(batch['indices'])}")
        print(f"  å—åˆ†å¸ƒå½¢çŠ¶: {batch['block_distributions'].shape}")
        print(f"  ç†è®ºåˆ†å¸ƒå½¢çŠ¶: {batch['theoretical_distributions'].shape}")
        print(f"  æ®‹å·®ç›®æ ‡å½¢çŠ¶: {batch['residual_targets'].shape}")
        print(f"  æ¡ä»¶ç‰¹å¾å½¢çŠ¶: {batch['condition_features'].shape}")
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        print(f"\næ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"  å—åˆ†å¸ƒæ±‚å’Œ: {torch.sum(batch['block_distributions'], dim=1)[:3]}")
        print(f"  ç†è®ºåˆ†å¸ƒæ±‚å’Œ: {torch.sum(batch['theoretical_distributions'], dim=1)[:3]}")
        print(f"  æ®‹å·®èŒƒå›´: [{torch.min(batch['residual_targets']):.4f}, {torch.max(batch['residual_targets']):.4f}]")
        
        print("âœ… æ•°æ®é›†æµ‹è¯•é€šè¿‡!")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_dataset()
